<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content=
      "width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Computational Physics Lecture 5</title>
    <link rel="stylesheet" href="../dist/reset.css">
    <link rel="stylesheet" href="../dist/reveal.css">
    <link rel="stylesheet" href="../dist/theme/serif.css"
      id="theme">
    <!-- <link rel="stylesheet" href="./dist/theme/night.css" id="theme"> -->
    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href=
      "../plugin/highlight/monokai.css" id="highlight-theme">
    <link rel="stylesheet" href=
      "../css/style.css">
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <h2>Numerical Differentiation</h2>
        </section>

        <section>
          <h2>Finite Difference</h2>
          <p>Given a function $f(x)$, we can approximate its derivative
            $f'(x)$ by the finite difference formula</p>
          $$
          f'(x) \approx \frac{f(x+h) - f(x)}{h}
          $$
          <p>where $h$ is a small number. This is called the <em>forward
            difference</em> formula. </p>
          <p>In the limit $h \to 0$, we expect the approximation to converge to
          the value of the derivative. The question is how quickly does it converge?</p>
        </section>

        <section>
          <h2>Finite Difference</h2>
          <p>Let's estimate the error due to forward difference. Start from the
          regular Taylor explansion:</p>
          $$
          f(x + h) = f(x) + hf'(x) + \frac{1}{2}h^2f''(x) + O(h^3)
          $$
          <p>We can rearrange this to get $f'(x)$ on the left hand side:</p>
          $$
          f'(x) = \frac{f(x+h) - f(x)}{h} - \frac{1}{2}hf''(x) + O(h^2)
          $$
          <p>The first term is our forward difference expression, while the rest
            becomes the <em>truncation error</em>.</p>
        </section>

        <section>
          <h2>Finite Difference</h2>
          <p>The error of forward difference is:</p>
          $$
          \varepsilon = \frac{1}{2}hf''(x) + O(h^2)
          $$
          <p>Since the largest term scales with first order in $h$, we say this
          is a first order method.</p>
        </section>

        <section>
          <h2>Finite Difference</h2>
          <p>We can achieve second order accuracy by using a slightly different formula:</p>
          $$
          f'(x) \approx \frac{f(x+h) - f(x-h)}{2h}
          $$
          <p>This scheme is called <em>central difference</em>.</p>
        </section>

        <section>
          <h2>Finite Difference</h2>
          <p>Expanding both $f(x+h)$ and $f(x-h)$ in Taylor series, we get:</p>
          $$
          \begin{align}
          f(x+h) &= f(x) + hf'(x) + \frac{1}{2}h^2f''(x) + \frac{1}{6}h^3f'''(x) + O(h^4) \\
          f(x-h) &= f(x) - hf'(x) + \frac{1}{2}h^2f''(x) - \frac{1}{6}h^3f'''(x) + O(h^4) \\
          \end{align}
          $$
          <p>We can construct $f'(x)$ from these two equations:</p>
          $$
          f'(x) = \frac{f(x+h) - f(x-h)}{2h} - \frac{1}{6}h^2f'''(x) + O(h^3)
          $$
        </section>

        <section>
          <h2>Finite Difference</h2>
          <p>The error of central difference is:</p>
          $$
          \varepsilon = -\frac{1}{6}h^2f'''(x) + O(h^3)
          $$
          <p>Since the largest term now scales with $h^2$, we call this scheme
          second-order accurate.</p>
        </section>

        <section>
          <h2>Higher Order Formulae</h2>
          <p>By using more points, we can construct higher order formulae. For
          example, the following formula is fourth order accurate:</p>
          $$
          f'(x) \approx \frac{-f(x+2h) + 8f(x+h) - 8f(x-h) + f(x-2h)}{12h}
          $$
        </section>

        <section>
          <h2>Higher Order Formulae</h2>
          <p>How to find formulae like this?</p>
          $$
          \begin{align}
          f(x+2h) &= f(x) + 2hf'(x) + \frac{4}{2}h^2f''(x) + \frac{8}{6}h^3f'''(x) + O(h^4) \\
          f(x+h) &= f(x) + hf'(x) + \frac{1}{2}h^2f''(x) + \frac{1}{6}h^3f'''(x) + O(h^4) \\
          f(x-h) &= f(x) - hf'(x) + \frac{1}{2}h^2f''(x) - \frac{1}{6}h^3f'''(x) + O(h^4) \\
          f(x-2h) &= f(x) - 2hf'(x) + \frac{4}{2}h^2f''(x) - \frac{8}{6}h^3f'''(x) + O(h^4) \\
          \end{align}
          $$
          <p>Solve for $f'(x)$</p>
        </section>

        <section>
          <h2>Finite Difference</h2>
          <p>The same prescription can be used to find higher derivatives too:</p>
          $$
          f''(x) \approx \frac{f(x+h) - 2f(x) + f(x-h)}{h^2}
          $$
          <p>The error on this estimate can be found to be:</p>
          $$
          \varepsilon = -\frac{1}{12}h^2f^{(4)}(x) + O(h^4)
          $$
          <p>So this formula is second-order accurate, similar to central difference.</p>
        </section>

        <section>
          <h2>Partial Derivatives</h2>
          <p>Finite difference can be used to calculate partial derivatives too.
          For example, the partial derivative of $f(x,y)$ with respect to $x$ and $y$ are:</p>
          $$
          \begin{align}
          \frac{\partial f}{\partial x} &\approx \frac{f(x+h,y) - f(x-h,y)}{2h} \\
          \frac{\partial f}{\partial y} &\approx \frac{f(x,y+h) - f(x,y-h)}{2h}
          \end{align}
          $$
          <p>This will be a cornerstone for our discussion of PDEs in the future.</p>
        </section>

        <section>
          <h2>Symbolic Differentiation</h2>
        </section>

        <section>
          <h2>Symbolic Differentiation</h2>
          <p>Symbolic differentiation is the process of finding the derivative
          of a function using its symbolic representation and rules of
          differentiation.</p>
          <ul>
            <li>$x^\alpha\,\longrightarrow\, \alpha x^{\alpha-1}$</li>
            <li>$\exp(x)\,\longrightarrow\, \exp(x)$</li>
            <li>$\log(x)\,\longrightarrow\, \frac{1}{x}$</li>
            <li>$f(x) + g(x)\,\longrightarrow\, f'(x) + g'(x)$</li>
            <li>$f(x)g(x)\,\longrightarrow\, f'(x)g(x) + f(x)g'(x)$</li>
            <li>$f(g(x))\,\longrightarrow\, f'(g(x))g'(x)$</li>
            <li>...</li>
          </ul>
        </section>

        <section>
          <h2>Symbolic Differentiation</h2>
          <p>A computer algebra system typically represents expressions as a
          tree of mathematical operations:</p>
          <img src="parse-tree.png" alt="tree" width="40%">
          <p>This gives the tree representation of the expression $(1 - x / 9) + 7\sin x$.</p>
        </section>

        <section>
          <h2>Symbolic Differentiation</h2>
          <p>Once you know the tree representation, you can apply the rules of
          differentiation to the tree to get the derivative.</p>
          <img src="parse-tree.png" alt="tree" width="40%">
          <p>The derivative of the tree depicted above is $-1/9 + 7\cos x$.</p>
        </section>

        <section>
          <h2>Symbolic Differentiation</h2>
          <p>Significant effort may be needed to simplify the final expression,
          especially when the original function is very complicated.</p>
          <p>Special functions that cannot be written in terms of elementary
          functions need their own special rules. E.g. $J_\nu(x)$, $H_n(x)$,
          $\mathrm{Erf}(x)$, etc.</p>
          <p>Symbolic differentiation is very useful for analytical work, but
           it's typically not directly used in numerical calculations.</p>
        </section>

        <section>
          <h2>Automatic Differentiation</h2>
        </section>

        <section>
          <h2>Automatic Differentiation</h2>
          <p>Automatic differentiation (AD) is a set of techniques to numerically
          evaluate the derivative of a function specified by a computer program.</p>
          <p>It applies the rules of differentiation on the floating point
            operations level, allowing the program to compute the derivative of a function as a <em>byproduct of function evaluation</em>,
            with a small overhead.</p>
        </section>

        <section>
          <h2>Automatic Differentiation</h2>
          <p>Let's look at an example. Consider the following function:</p>
          $$
          f(x_1, x_2) = \left[\sin\left(\frac{x_1}{x_2}\right) + \frac{x_1}{x_2} - e^{x_1}\right]\times \left[\frac{x_2}{x_1} - e^{x_2}\right]
          $$
          <p>Let's try to simultaneously calculate $f(1.5, 0.5)$ and $\partial f/\partial x_1(1.5, 0.5)$</p>
        </section>

        <section>
          <h2>Automatic Differentiation</h2>
          <table>
            <tr>
              <td>$v_0 = x_1 = 1.5$</td>
              <td>$\dot{v}_0 = 1$</td>
            </tr>
            <tr>
              <td>$v_1 = x_2 = 0.5$</td>
              <td>$\dot{v}_1 = 0$</td>
            </tr>
            <tr>
              <td>$v_2 = v_0 / v_1 = 3$</td>
              <td>$\dot{v}_2 = (\dot{v}_0 v_1 - v_0 \dot{v}_1) / v_1^2 = 2$</td>
            </tr>
            <tr>
              <td>$v_3 = \sin v_2 = 0.141$</td>
              <td>$\dot{v}_3 = \dot{v}_2 \cos v_2 = -1.980$</td>
            </tr>
            <tr>
              <td>$v_4 = \exp(v_0) = 4.482$</td>
              <td>$\dot{v}_4 = \dot{v}_0 \exp(v_0) = 4.482$</td>
            </tr>
            <tr>
              <td>$v_5 = v_1 / v_0 = 0.333$</td>
              <td>$\dot{v}_5 = (\dot{v}_1 v_0 - v_1 \dot{v}_0) / v_0^2 = -0.222$</td>
            </tr>
            <tr>
              <td>$v_6 = \exp(v_1) = 1.649$</td>
              <td>$\dot{v}_6 = \dot{v}_1 \exp(v_1) = 0$</td>
            </tr>
            <tr>
              <td>$v_7 = v_3 + v_2 - v_4 = -1.341$</td>
              <td>$\dot{v}_7 = \dot{v}_3 + \dot{v}_2 - \dot{v}_4 = -4.480$</td>
            </tr>
            <tr>
              <td>$v_8 = v_5 - v_6 = -1.316$</td>
              <td>$\dot{v}_8 = \dot{v}_5 - \dot{v}_6 = -0.222$</td>
            </tr>
            <tr>
              <td>$f(x_1, x_2) = v_7 \times v_8 = 1.765$</td>
              <td>$\partial f/\partial x_1 = \dot{v}_7 \times v_8 + v_7 \times \dot{v}_8 = 6.193$</td>
            </tr>
          </table>
        </section>

        <section>
          <h2>Automatic Differentiation</h2>
          <p>Note that the derivative calculated using AD is accurate to machine
            precision. It is significantly more accurate than typical finite
            difference results.</p>
          <p>AD is also significantly faster than symbolic differentiation, as
          it only adds a constant amount of overhead to every floating point
          operation.</p>
          <p>This technique is now widely used to compute derivatives of a deep
          neural network. It facilitates gradient descent, which is used to
          train neural networks towards a local extremum.</p>
        </section>

      </div>
    </div>
    <!-- <script src="js/head.min.js"></script> -->
    <!-- <script>
         head.js(
         "vendor/jquery.min.js",
         // "vendor/three.js/build/three.min.js",
         // "vendor/three.js/examples/js/controls/OrbitControls.js",
         /* "vendor/socket.io-client/dist/socket.io.js", */
         // "vendor/THREE.MeshLine.js",
         /* "vendor/dat.gui.min.js",*/
         "vendor/EventEmitter.js",
         // "js/samples.js",
         );
         </script> -->
    <script src="../dist/reveal.js"></script>
    <script src="../plugin/math/math.js"></script>
    <script src="../plugin/notes/notes.js"></script>
    <script src="../plugin/markdown/markdown.js"></script>
    <script src="../plugin/highlight/highlight.js"></script>

    <script>
      // More info about initialization & config:
      // - https://revealjs.com/initialization/
      // - https://revealjs.com/config/
      Reveal.initialize({
        transition: "fade",
        slideNumber: true,
        history: true,
        center: true,
        width: "100%",
        height: "100%",
        disableLayout: false,
        navigationMode: "default",
        // minScale: 1,
        // maxScale: 1,
        margin: 0.2,
        padding: 0,
        hash: true,
        math: {
          mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          // pass other options into `MathJax.Hub.Config()`
          TeX: { Macros: { RR: "{\\bf R}" } }
        },
        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath ]
      });
      Reveal.configure({
        keyboard: {
          81: null, // Do not respond to Q
          // 32: null, // Do not respond to space
        }
      });
      // Reveal.addEventListener("slidechanged", function(event) {
      //   var foot1 = document.getElementById("footer1");
      //   var foot2 = document.getElementById("footer2");
      //
      //   if (Reveal.getIndices().h === 0) {
      //     foot1.style.display = "none";
      //     foot2.style.display = "none";
      //   } else {
      //     foot1.style.display = "block";
      //     foot2.style.display = "block";
      //   }
      // });
      //
    </script>
  </body>
</html>
