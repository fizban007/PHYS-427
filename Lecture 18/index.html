<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content=
      "width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Computational Physics Lecture 18</title>
    <link rel="stylesheet" href="../dist/reset.css">
    <link rel="stylesheet" href="../dist/reveal.css">
    <link rel="stylesheet" href="../dist/theme/serif.css"
      id="theme">
    <!-- <link rel="stylesheet" href="./dist/theme/night.css" id="theme"> -->
    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href=
      "../plugin/highlight/monokai.css" id="highlight-theme">
    <link rel="stylesheet" href=
      "../css/style.css">
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <h2>Numerical Linear Algebra</h2>
        </section>

        <section>
          <h2>Cholesky Decomposition</h2>
          <p>When the matrix $\mathbf{A}$ is symmetric and positive-definite, we
          can define a special form of LU decomposition.</p>
          <ul>
            <li>Symmetric means that $a_{ij} = a_{ji}$</li>
            <li>Positive-definite means that for any vector $\mathbf{v}$, $\mathbf{v}\cdot\mathbf{A}\cdot\mathbf{v} > 0$. It
            is also equivalent to $\mathbf{A}$ having all positive eigenvalues</li>
          </ul>
        </section>

        <section>
          <h2>Cholesky Decomposition</h2>
          <p>When the matrix $\mathbf{A}$ is symmetric and positive-definite, we
          can define a special form of LU decomposition:</p>
          $$
          \mathbf{L}\mathbf{L}^T = \mathbf{A}
          $$
          <p>where $\mathbf{L}$ is a lower triangular matrix. This is a special
          case of LU decomposition where $\mathbf{U}$ is simply the transpose of
          $\mathbf{L}$.</p>
        </section>

        <section>
          <h2>Cholesky Decomposition</h2>
          <p>The components of $\mathbf{L}$ can be found using the following
          algorithm similar to Crout's algorithm:</p>
          $$
          \begin{align}
          L_{ii} &= \left(A_{ii} - \sum_{k=1}^{i-1}L_{ik}^2\right)^{1/2} \\
          L_{ji} &= \frac{1}{L_{ii}}\left(A_{ji} - \sum_{k=1}^{i-1}L_{jk}L_{ik}\right), \quad j > i
          \end{align}
          $$
          <p class="fragment">This is about a factor of 2 faster than normal LU decomposition. The
          algorithm is also numerically stable, without the need of pivoting.</p>
          <p class="fragment">When the algorithm fails, it means the matrix $\mathbf{A}$ is not
          positive-definite. It is a relatively efficient way to test whether a symmetric matrix is positive-definite.</p>
        </section>

        <section>
          <h2>Cholesky Decomposition</h2>
          <p>Cholesky decomposition is useful for solving linear equations, just like LU
          decomposition. The only difference is that now $\mathbf{U}$ is the
          transpose of $\mathbf{L}$.</p>
          <p class="fragment">You can also think of Cholesky decomposition as
          finding the "square root" of the matrix $\mathbf{A}$. It is useful in
          simulating correlated random variables.</p>
        </section>

        <section>
          <h2>QR Decomposition</h2>
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>Another useful matrix factorization method is the QR decomposition
          (NOT related to "QR codes"):</p>
          $$
          \mathbf{A} = \mathbf{Q}\mathbf{R}
          $$
          <p>where $\mathbf{Q}$ is an orthogonal matrix and $\mathbf{R}$ is an
          upper triangular matrix.</p>
          <p class="fragment">
            Remember that an orthogonal matrix is a matrix whose columns are
            orthonormal vectors. This means that $\mathbf{Q}^T\mathbf{Q} =
            \mathbf{I}$.
          </p>
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>Suppose we know the QR decomposition of a matrix, we can use it to
          find the solution to the system of linear equations
          $\mathbf{A}\mathbf{x} = \mathbf{Q}\mathbf{R}\mathbf{x} = \mathbf{b}$.</p>
          <div class="fragment">
          <p>Since $\mathbf{Q}^T$ is the inverse of
          $\mathbf{Q}$, we can multiply both sides by $\mathbf{Q}^T$:</p>
          $$
          \mathbf{R}\mathbf{x} = \mathbf{Q}^T\mathbf{b}
          $$
          <p class="fragment">
            Because $\mathbf{R}$ is upper triangular, we can solve for
            $\mathbf{x}$ using back substitution.
          </p>
          </div>
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>QR decomposition can also be used to determine the eigenvalues of a
          matrix $\mathbf{A}$.</p>
          <div class="fragment">
            <p>Suppose we already know the QR decomposition of the matrix,
            $\mathbf{A} = \mathbf{Q}\mathbf{R}$, we can form the following combination:</p>
            $$
            \mathbf{A}_1 = \mathbf{R}\mathbf{Q} = \mathbf{Q}^T\mathbf{A}\mathbf{Q}
            $$
          </div>
          <div class="fragment">
            <p>This is a similarity transformation that preserves the
            eigenvalues of the original matrix.</p>
          </div>
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>Suppose we now find the QR decomposition of $\mathbf{A}_1$ and repeat this process:
          </p>
          $$
          \begin{align}
          \mathbf{A}_1 &= \mathbf{Q}_1 \mathbf{R}_1 \\
          \mathbf{A}_2 &= \mathbf{R}_1 \mathbf{Q}_1 = \mathbf{Q}_1^T\mathbf{A}_1\mathbf{Q}_1 \\
                       &= \mathbf{Q}_1^T\mathbf{Q}^T \mathbf{A} \mathbf{Q}\mathbf{Q}_1^T \\
          \dots
          \end{align}
          $$
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>It can be shown that this process eventually will converge. After
          many steps, the off-diagonal terms of $\mathbf{A}_n$ will become very
          small. Then $\mathbf{A}_n$ is now approximately diagonal, we can read
          off its eigenvalues from its diagonal components.</p>
          <div class="fragment">
            <p>Quite commonly, the iteration will not yield a completely
            diagonal matrix, but will arrive at an upper-triangular matrix. This
              is called the <em>Schur form</em> of $\mathbf{A}$, and the
              diagonal elements still correspond to the eigenvalues of $\mathbf{A}$.</p>
          </div>
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>Let's see an example. Consider the following matrix</p>
          $$
          \mathbf{A} = \begin{pmatrix}
          4 & 2 & 1 \\
          2 & 1 & 0 \\
          1 & 0 & 3
          \end{pmatrix}
          $$
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>Its QR decomposition is (found by Mathematica):</p>
          $$
          \mathbf{A} = \mathbf{Q}_0\mathbf{R}_0 = \begin{pmatrix}
 0.87& 0.44& 0.22\\
 0.20& 0.10& -0.98\\
 0.45& -0.89& 0.
          \end{pmatrix}
          \begin{pmatrix}
 4.58& 2.18& 1.52\\
 0. & 0.49& -2.73\\
 0. & 0. & 0.45
          \end{pmatrix}
          $$
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>We proceed to form $\mathbf{A}_{k+1} = \mathbf{R}_k\mathbf{Q}_k$,
          and iterate. After 9 iterations, the resulting matrix is:</p>
          $$
          \mathbf{A}_9 = \begin{pmatrix}
 5.22& -0.13& 1.16\\
 0.003& 2.71& 0.21\\
 2.82\times 10^{-19} & -2.39\times 10^{-15} & -0.07\\
          \end{pmatrix}
          $$
          <div class="fragment">
            <p>The actual eigenvalues of the original matrix $\mathbf{A}$ are:</p>
            $$
            \lambda_1, \lambda_2, \lambda_3 = 5.35, 2.72, -0.07
            $$
          </div>
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>In practice, iterating $\mathbf{A}_{k+1} =
          \mathbf{R}_k\mathbf{Q}_k$ can be incredibly slow to converge. The
          often-used strategy is to first reduce the matrix to tridiagonal form,
          then apply this QR algorithm.</p>
        </section>

        <section>
          <h2>Computing QR Decomposition</h2>
          <p>How do we compute the QR decomposition of a given matrix?</p>
          <div class="fragment">
            <p>One can use the Gram-Schmidt process. Think of the original
            matrix as a set of $n$ individual columns:</p>
            $$
            \mathbf{A} = \begin{pmatrix}
            | & | & \dots & | \\
            \mathbf{a}_1 & \mathbf{a}_2 & \dots & \mathbf{a}_n \\
            | & | & \dots & |
            \end{pmatrix}
            $$
          </div>
        </section>

        <section>
          <h2>Gram-Schmidt Process</h2>
          <p>Let's define a set of orthogonal vectors:</p>
          $$
          \begin{array}{ll}
          \mathbf{u}_1 = \mathbf{a}_1,& \mathbf{e}_1 = \mathbf{u}_1/|\mathbf{u}_1| \\
          \mathbf{u}_2 = \mathbf{a}_2 - (\mathbf{e}_1\cdot\mathbf{a}_2)\mathbf{e}_1,& \mathbf{e}_2 = \mathbf{u}_2/|\mathbf{u}_2| \\
          \mathbf{u}_3 = \mathbf{a}_3 - (\mathbf{e}_1\cdot\mathbf{a}_3)\mathbf{e}_1 - (\mathbf{e}_2\cdot\mathbf{a}_3)\mathbf{e}_2,& \mathbf{e}_3 = \mathbf{u}_3/|\mathbf{u}_3| \\
          \ldots & \\
          \mathbf{u}_i = \mathbf{a}_i - \sum_{j=1}^{i-1}(\mathbf{e}_j\cdot\mathbf{a}_i)\mathbf{e}_j,& \mathbf{e}_i = \mathbf{u}_i/|\mathbf{u}_i| \\
          \end{array}
          $$
        </section>

        <section>
          <h2>Gram-Schmidt Process</h2>
          <p>Rearranging the definition of these vectors, we can write the
          original vectors of $\mathbf{A}$ as:</p>
          $$
          \begin{align}
          \mathbf{a}_1 &= |\mathbf{u}_1|\mathbf{e}_1 \\
          \mathbf{a}_2 &= |\mathbf{u}_2|\mathbf{e}_2 + (\mathbf{e}_1\cdot\mathbf{a}_2)\mathbf{e}_1 \\
          \mathbf{a}_3 &= |\mathbf{u}_3|\mathbf{e}_3 + (\mathbf{e}_1\cdot\mathbf{a}_3)\mathbf{e}_1 + (\mathbf{e}_2\cdot\mathbf{a}_3)\mathbf{e}_2 \\
          \ldots
          \end{align}
          $$
          <p class="fragment">
            This set of equations can be written in matrix form.
          </p>
        </section>

        <section>
          <h2>Gram-Schmidt Process</h2>
          $$
          \begin{pmatrix}
          | & | & \dots & | \\
          \mathbf{a}_1 & \mathbf{a}_2 & \dots & \mathbf{a}_n \\
          | & | & \dots & | \\
          | & | & \dots & |
          \end{pmatrix}
          =
          \begin{pmatrix}
          | & | & \dots & | \\
          \mathbf{e}_1 & \mathbf{e}_2 & \dots & \mathbf{e}_n \\
          | & | & \dots & | \\
          | & | & \dots & |
          \end{pmatrix}
          \begin{pmatrix}
          |\mathbf{u}_1| & (\mathbf{e}_1\cdot\mathbf{a}_2) & \dots & (\mathbf{e}_1\cdot\mathbf{a}_n) \\
          0 & |\mathbf{u}_2| & \dots & (\mathbf{e}_2\cdot\mathbf{a}_n) \\
          \vdots & \vdots & \ddots & \vdots \\
          0 & 0 & \dots & |\mathbf{u}_n|
          \end{pmatrix}
          $$
          <p class="fragment">The matrices on the right hand side are our
          $\mathbf{Q}$ and $\mathbf{R}$. $\mathbf{Q}$ is an orthogonal matrix
          because the set of vectors $\mathbf{e}_i$ is orthonormal, $\mathbf{e}_i\cdot\mathbf{e}_j = \delta_{ij}$.</p>
        </section>

        <section>
          <h2>Gram-Schmidt Process</h2>
          <p>Unfortunately the Gram-Schmidt process is numerically unstable, especially
          for large matrices. The often implemented QR decomposition algorithm
            is instead based on <em>Householder transformations</em>.</p>
          <p class="fragment">The Gram-Schmidt process uses repeated projections
            to get rid of undesired components of the matrix. Householder transformations use <em>reflections</em> instead.</p>
        </section>

        <section>
          <h2>Householder Transformations</h2>
          <p>For a column vector $\mathbf{x}$, the goal is to set all its
          components to zero except the first one, using an orthogonal transformation.</p>
          <div class="fragment">
            <p>This is achieved using the Householder matrix:</p>
            $$
            \mathbf{P} = \mathbf{1} - 2\frac{\mathbf{u}\cdot\mathbf{u}^T}{|\mathbf{u}|^2},
            \quad \mathbf{u} = \begin{pmatrix}
            x_1 - |\mathbf{x}| \\
            x_2 \\
            \vdots \\
            x_n
            \end{pmatrix}
            $$
          </div>
          <div class="fragment">
            <p>The matrix $\mathbf{P}$ is orthogonal, and is its own inverse.
              $\mathbf{P} = \mathbf{P}^{-1} = \mathbf{P}^T$.</p>
          </div>
        </section>

        <section>
          <h2>Householder Transformations</h2>
          <p>To set all the lower elements of the first column $\mathbf{a}_1$ to
          zero, we take $\mathbf{x} = \mathbf{a}_1$ to form our first
          Householder matrix $\mathbf{P}_1$, then multiply it to $\mathbf{A}$.</p>
          <div class="fragment">
            <p>Then, to set the lower elements of the second column
            $\mathbf{a}_2$ to zero, we take the $\mathbf{x}$ to be the lower
            $n-1$ elements of $\mathbf{a}_2$ and form a $(n-1)\times(n-1)$
            Householder matrix $\mathbf{P}_2$.</p>
          </div>
        </section>

        <section>
          <h2>Householder Transformations</h2>
          <p>Householder transformation is the most widely implemented algorithm
          for QR decomposition.</p>
          <p class="fragment">For example, the <code>Eigen</code> C++ library contains the <a href="https://eigen.tuxfamily.org/dox/classEigen_1_1HouseholderQR.html"><code>HouseholderQR</code></a> module
            that calculates the QR decomposition of a matrix using Householder
            transformations. See <a href="https://eigen.tuxfamily.org/dox/group__TutorialLinearAlgebra.html">this page</a> for
            a comparison of various decompositions for solving linear equations.</p>
          <p class="fragment">
            The Python library <a href="https://numpy.org/devdocs/reference/generated/numpy.linalg.qr.html"><code>numpy.linalg.qr</code></a> wraps
            the LAPACK function <code>dgeqrf</code>. It is also based on the
            Householder algorithm.
          </p>
        </section>

        <section>
          <h2>Householder Transformations</h2>
          <p>One can also use a series of Householder transformations to bring a
          symmetric matrix to tri-diagonal form, then use the
          $\mathbf{R}\mathbf{Q}$ iteration method to find its eigenvalues.</p>
          <p class="fragment">
            This is a much more efficient way to compute the eigenvalues of a
            matrix than brute-force iteration with QR decomposition.
          </p>
          <p class="fragment">
            In practice, use a reputable library when you want to compute the
            eigenvalues and/or eigenvectors of a matrix. For example, Python has <code>numpy.linalg.eig</code> for
            a general $n\times n$ matrix, and <code>numpy.linalg.eigh</code> for
            a real symmetric matrix or complex Hermitian matrix.
          </p>
        </section>

      </div>
    </div>
    <!-- <script src="js/head.min.js"></script> -->
    <!-- <script>
         head.js(
         "vendor/jquery.min.js",
         // "vendor/three.js/build/three.min.js",
         // "vendor/three.js/examples/js/controls/OrbitControls.js",
         /* "vendor/socket.io-client/dist/socket.io.js", */
         // "vendor/THREE.MeshLine.js",
         /* "vendor/dat.gui.min.js",*/
         "vendor/EventEmitter.js",
         // "js/samples.js",
         );
         </script> -->
    <script src="../dist/reveal.js"></script>
    <script src="../plugin/math/math.js"></script>
    <script src="../plugin/notes/notes.js"></script>
    <script src="../plugin/markdown/markdown.js"></script>
    <script src="../plugin/highlight/highlight.js"></script>

    <script>
      // More info about initialization & config:
      // - https://revealjs.com/initialization/
      // - https://revealjs.com/config/
      Reveal.initialize({
        transition: "fade",
        slideNumber: true,
        history: true,
        center: true,
        width: "100%",
        height: "100%",
        disableLayout: false,
        navigationMode: "default",
        // minScale: 1,
        // maxScale: 1,
        margin: 0.2,
        padding: 0,
        hash: true,
        math: {
          mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          // pass other options into `MathJax.Hub.Config()`
          TeX: { Macros: { RR: "{\\bf R}" } }
        },
        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath ]
      });
      Reveal.configure({
        keyboard: {
          81: null, // Do not respond to Q
          // 32: null, // Do not respond to space
        }
      });
      // Reveal.addEventListener("slidechanged", function(event) {
      //   var foot1 = document.getElementById("footer1");
      //   var foot2 = document.getElementById("footer2");
      //
      //   if (Reveal.getIndices().h === 0) {
      //     foot1.style.display = "none";
      //     foot2.style.display = "none";
      //   } else {
      //     foot1.style.display = "block";
      //     foot2.style.display = "block";
      //   }
      // });
      //
    </script>
  </body>
</html>
