<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content=
      "width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Computational Physics Lecture 17</title>
    <link rel="stylesheet" href="../dist/reset.css">
    <link rel="stylesheet" href="../dist/reveal.css">
    <link rel="stylesheet" href="../dist/theme/serif.css"
      id="theme">
    <!-- <link rel="stylesheet" href="./dist/theme/night.css" id="theme"> -->
    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href=
      "../plugin/highlight/monokai.css" id="highlight-theme">
    <link rel="stylesheet" href=
      "../css/style.css">
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <h2>Numerical Linear Algebra</h2>
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>Remember Gaussian elimination ends up giving you an upper
          triangular matrix:</p>
          <p>$$
            \begin{pmatrix}
              u_{11} & u_{12} & u_{13} & \dots & u_{1n} \\
              0 & u_{22} & u_{23} & \dots & u_{2n} \\
              0 & 0 & u_{33} & \dots & u_{3n} \\
              \vdots & \vdots & \vdots & \ddots & \vdots \\
              0 & 0 & 0 & \dots & u_{nn}
            \end{pmatrix}
          $$</p>
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>We can formalize this process using matrix multiplications. The
          first step of Gaussian elimination is to get rid of the first column (except the diagonal element):</p>
          <div style="font-size: 80%">
          $$
            \frac{1}{a_{11}}
            \begin{pmatrix}
              a_{11} & 0 & 0 & \dots & 0 \\
              -a_{21} & a_{11} & 0 & \dots & 0 \\
              -a_{31} & 0 & a_{11} & \dots & 0 \\
              \vdots & \vdots & \vdots & \ddots & \vdots \\
              -a_{n1} & 0 & 0 & \dots & a_{11}
            \end{pmatrix}
            \begin{pmatrix}
              a_{11} & a_{12} & a_{13} & \dots & a_{1n} \\
              a_{21} & a_{22} & a_{23} & \dots & a_{2n} \\
              a_{31} & a_{32} & a_{33} & \dots & a_{3n} \\
              \vdots & \vdots & \vdots & \ddots & \vdots \\
              a_{n1} & a_{n2} & a_{n3} & \dots & a_{nn}
            \end{pmatrix}
            =
            \begin{pmatrix}
              a_{11} & a_{12} & a_{13} & \dots & a_{1n} \\
              0 & a'_{22} & a'_{23} & \dots & a'_{2n} \\
              0 & a'_{32} & a'_{33} & \dots & a'_{3n} \\
              \vdots & \vdots & \vdots & \ddots & \vdots \\
              0 & a'_{n2} & a'_{n3} & \dots & a'_{nn}
            \end{pmatrix}
          $$
          </div>
          <p>We denote the lower-triangular matrix $\mathbf{L}_1$.</p>
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>The next step of Gaussian elimination can be written in a similar way:</p>
          <div style="font-size: 80%">
            $$
              \frac{1}{a'_{22}}
              \begin{pmatrix}
                a'_{22} & 0 & 0 & \dots & 0 \\
                0 & a'_{22} & 0 & \dots & 0 \\
                0 & -a'_{32} & a'_{22} & \dots & 0 \\
                \vdots & \vdots & \vdots & \ddots & \vdots \\
                0 & -a'_{n2} & 0 & \dots & a'_{22}
              \end{pmatrix}
              \begin{pmatrix}
                a'_{11} & a'_{12} & a'_{13} & \dots & a'_{1n} \\
                0 & a'_{22} & a'_{23} & \dots & a'_{2n} \\
                0 & a'_{32} & a'_{33} & \dots & a'_{3n} \\
                \vdots & \vdots & \vdots & \ddots & \vdots \\
                0 & a'_{n2} & a'_{n3} & \dots & a'_{nn}
              \end{pmatrix}
              =
              \begin{pmatrix}
                a'_{11} & a'_{12} & a'_{13} & \dots & a'_{1n} \\
                0 & a'_{22} & a'_{23} & \dots & a'_{2n} \\
                0 & 0 & a''_{33} & \dots & a''_{3n} \\
                \vdots & \vdots & \vdots & \ddots & \vdots \\
                0 & 0 & a''_{n3} & \dots & a''_{nn}
              \end{pmatrix}
            $$
          </div>
          <p>We denote the lower-triangular matrix $\mathbf{L}_2$.</p>
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>After $n-1$ steps, we get:</p>
          $$
            \mathbf{L}_{n-1}\dots\mathbf{L}_2\mathbf{L}_1\mathbf{A} = \mathbf{U}
          $$
          <p>where $\mathbf{U}$ is upper-triangular.</p>
          <div class="fragment">
            <p>Up to now, this is simply a formalization of Gaussian elimination.
            If all the matrices $\mathbf{L}_i$ are invertible, we can write:</p>
            $$
              \mathbf{A} = \left(\mathbf{L}_1^{-1}\mathbf{L}_2^{-1}\dots\mathbf{L}_{n-1}^{-1}\right)\mathbf{U}
            $$
          </div>
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>In fact, the inverse of the lower-triangular matrices are lower-triangular too:</p>
          <div style="font-size: 80%">
            $$
              \mathbf{L}_1^{-1} =
              \begin{pmatrix}
                1 & 0 & 0 & \dots & 0 \\
                -a_{21}/a_{11} & 1 & 0 & \dots & 0 \\
                -a_{31}/a_{11} & 0 & 1 & \dots & 0 \\
                \vdots & \vdots & \vdots & \ddots & \vdots \\
                -a_{n1}/a_{11} & 0 & 0 & \dots & 1
              \end{pmatrix}^{-1}
              =
              \begin{pmatrix}
                1 & 0 & 0 & \dots & 0 \\
                a_{21}/a_{11} & 1 & 0 & \dots & 0 \\
                a_{31}/a_{11} & 0 & 1 & \dots & 0 \\
                \vdots & \vdots & \vdots & \ddots & \vdots \\
                a_{n1}/a_{11} & 0 & 0 & \dots & 1
              \end{pmatrix}
            $$
          </div>
          <div class="fragment">
          <p>The product of the inverses of the lower triangular matrices
            $\mathbf{L}_i$ is again lower triangular, so we can write:</p>
          $$
            \mathbf{A} = \left(\mathbf{L}_1^{-1}\mathbf{L}_2^{-1}\dots\mathbf{L}_{n-1}^{-1}\right) = \mathbf{L}\mathbf{U}
          $$
          <p>This is called the <em>LU decomposition</em> (or factorization) of $\mathbf{A}$.</p>
          </div>
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>Typically, it is possible to take the diagonal elements of the
            lower-triangular matrix $\mathbf{L}$ to be all 1. In that case, we
            can write all the elements of both $\mathbf{L}$ and $\mathbf{U}$ neatly in
            a single matrix:</p>
          $$
          \begin{pmatrix}
          u_{11} & u_{12} & u_{13} & \dots & u_{1n} \\
          l_{21} & u_{22} & u_{23} & \dots & u_{2n} \\
          l_{31} & l_{32} & u_{33} & \dots & u_{3n} \\
          \vdots & \vdots & \vdots & \ddots & \vdots \\
          l_{n1} & l_{n2} & l_{n3} & \dots & u_{nn}
          \end{pmatrix}
          $$
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>A simple algorithm exists to find all the coefficients systematically. It's called <em>Crout's algorithm</em>.
          For each column with index $j$, we proceed from top to bottom:</p>
          <div style="font-size: 80%">
            $$
            \begin{align}
              u_{1j} &= a_{1j} \\
              u_{ij} &= a_{ij} - \sum_{k=1}^{i-1}l_{ik}u_{kj} & \text{for } i = 2, \dots, j \\
              l_{ij} &= \frac{1}{u_{jj}}\left(a_{ij} - \sum_{k=1}^{j-1}l_{ik}u_{kj}\right) & \text{for } i = j + 1, \dots, n
            \end{align}
            $$
          </div>
          <p class="fragment">Partial pivoting can be implemented by "promoting"
          one of the $l_{ij}$ to the diagonal.</p>
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>It turns out we can always write a square matrix $\mathbf{A}$ as a
          product of a lower-triangular matrix $\mathbf{L}$ and an
          upper-triangular matrix $\mathbf{U}$ (potentially with some pivoting):</p>
          $$
            \mathbf{P}\mathbf{A} = \mathbf{L}\mathbf{U}
          $$
          <p>This is called LU decomposition with partial pivoting, or LUP. For
          proof, see e.g. <a href="https://arxiv.org/abs/math/0506382">arXiv:math/0506382</a>.</p>
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>Without pivoting, it is not guaranteed that the LU decomposition of
          matrix $\mathbf{A}$ exists. For example:</p>
          $$
          \begin{pmatrix}
          0 & 1 \\
          1 & 0
          \end{pmatrix} \neq \mathbf{L}\mathbf{U}
          $$
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>LU decomposition can be used for solving linear equations. The matrix equation:</p>
          $$
            \mathbf{A}\mathbf{x} = \mathbf{L}\mathbf{U}\mathbf{x} = \mathbf{b}
          $$
          <p>can be solved in two steps:</p>
          $$
            \mathbf{L}\mathbf{y} = \mathbf{b}
          $$
          $$
            \mathbf{U}\mathbf{x} = \mathbf{y}
          $$
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>We solve the first step using <em>forward substitution</em>:</p>
          $$
          \begin{align}
            y_1 &= \frac{b_1}{l_{11}} \\
            y_2 &= \frac{b_2 - l_{21}y_1}{l_{22}} \\
            \dots \\
            y_i &= \frac{1}{l_{ii}}\left(b_i - \sum_{j=1}^{i-1}l_{ij}y_j\right)
          \end{align}
          $$
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>Then we solve the second step using <em>backward substitution</em>:</p>
          $$
          \begin{align}
            x_n &= \frac{y_n}{u_{nn}} \\
            x_{n-1} &= \frac{y_{n-1} - u_{n-1,n}x_n}{u_{n-1,n-1}} \\
            \dots \\
            x_i &= \frac{1}{u_{ii}}\left(y_i - \sum_{j=i+1}^{n}u_{ij}x_j\right)
          \end{align}
          $$
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>Once we find the LU decomposition of matrix $\mathbf{A}$, we can
            use it to solve equations with any right hand side $\mathbf{b}$.</p>
          <div class="fragment">
            <p>For example, we can successively solve $n$ matrix equations with
            right hand side $\mathbf{b}_i$ from the identity matrix. This gives
            us the inverse of $\mathbf{A}$.</p>
          </div>
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>It is also straightforward to find the determinant of $\mathbf{A}$, which is simply:</p>
          $$
          \det \mathbf{A} = (-1)^p\prod_{i=1}^{N} u_{ii}
          $$
          <p>where $p$ is the number of row exchanges we carried out with partial pivoting.</p>
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>The Python function <a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html"><code>numpy.linalg.solve</code></a> wraps the LAPACK function <code>GESV</code>,
          which solves a linear system of equations
          $\mathbf{A}\mathbf{x}=\mathbf{b}$ using the LU decomposition.</p>
          <div class="fragment">
            <p>In general, <code>GESV</code> specifically uses LU decomposition with partial pivoting. It is only numerically stable for <em>invertible</em> square
            matrices.</p>
            <p class="fragment">
              The scipy version <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve.html"><code>scipy.linalg.solve</code></a> can take a few extra parameters. One of them is <code>assume_a</code>,
              which can be used to specify if the matrix $\mathbf{A}$ is
              symmetric, hermitian, or positive definite. Then it will call
              specific routines in LAPACK to handle these specific cases.
            </p>
          </div>
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>In the C++ library Eigen, there is a function called <a href="https://eigen.tuxfamily.org/dox/classEigen_1_1FullPivLU.html"><code>FullPivLU</code></a>,
            which computes the LU decomposition of a given matrix with full
            (including column) pivoting. It can work on matrices that are not
            invertible, and can reveal the rank of the matrix.</p>
        </section>

        <section>
          <h2>Tri-diagonal Matrices</h2>
          <p>The special case of tri-diagonal matrices is particularly simple to
          solve.</p>
          $$
          \begin{pmatrix}
          b_0 & c_0 & 0 & \dots & 0 & 0 \\
          a_1 & b_1 & c_1 & \dots & 0 & 0 \\
          0 & a_2 & b_2 & \dots & 0 & 0 \\
          \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
          0 & 0 & 0 & \dots & b_{n-2} & c_{n-2} \\
          0 & 0 & 0 & \dots & a_{n-1} & b_{n-1}
          \end{pmatrix}
          $$
        </section>

        <section>
          <h2>Tri-diagonal Matrices</h2>
          <p>Instead of storing the whole matrix with many zeros, we can
          store the vectors $\mathbf{a}$, $\mathbf{b}$, and $\mathbf{c}$.</p>
          <div class="fragment">
          <p>The LU decomposition of a tridiagonal matrix can be written as:</p>
          <div style="font-size: 80%;">
          $$
          \begin{pmatrix}
          b_0 & c_0 & 0 & \dots & 0 & 0 \\
          a_1 & b_1 & c_1 & \dots & 0 & 0 \\
          0 & a_2 & b_2 & \dots & 0 & 0 \\
          \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
          0 & 0 & 0 & \dots & b_{n-2} & c_{n-2} \\
          0 & 0 & 0 & \dots & a_{n-1} & b_{n-1}
         \end{pmatrix}
          =
          \begin{pmatrix}
         1 & 0 & 0 & \dots & 0 & 0 \\
          l_1 & 1 & 0 & \dots & 0 & 0 \\
          0 & l_2 & 1 & \dots & 0 & 0 \\
          \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
          0 & 0 & 0 & \dots & 1 & 0 \\
          0 & 0 & 0 & \dots & l_{n-1} & 1
          \end{pmatrix}
          \begin{pmatrix}
          u_0 & c_0 & 0 & \dots & 0 & 0 \\
          0 & u_1 & c_1 & \dots & 0 & 0 \\
          0 & 0 & u_2 & \dots & 0 & 0 \\
          \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
          0 & 0 & 0 & \dots & u_{n-2} & c_{n-2} \\
          0 & 0 & 0 & \dots & 0 & u_{n-1}
          \end{pmatrix}
          $$
          </div>
          </div>
        </section>

        <section>
          <h2>Tri-diagonal Matrices</h2>
          <p>It is easy to find the coefficients $l_i$ and $u_i$:</p>
          $$
          \begin{align}
          u_0 &= b_0 \\
          l_i &= a_i / u_{i-1} & \text{for } i = 1, \dots, n-1 \\
          u_i &= b_i - l_i c_{i-1} & \text{for } i = 1, \dots, n-1
          \end{align}
          $$
        </section>

        <section>
          <h2>Tri-diagonal Matrices</h2>
          <p>Then we still solve the linear system $\mathbf{A}\mathbf{x}=\mathbf{r}$ in two steps:</p>
          $$
          \mathbf{L}\mathbf{y} = \mathbf{r},\quad \mathbf{U}\mathbf{x} = \mathbf{y}
          $$
          <p>But each step can be written straightforwardly:</p>
          $$
          \begin{align}
          y_0 &= r_0 \\
          y_i &= r_i - l_i y_{i-1} & \text{for } i = 1, \dots, n-1 \\
          x_{n-1} &= y_{n-1} / u_{n-1} \\
          x_i &= \frac{y_i - c_i x_{i+1}}{u_i} & \text{for } i = n-2, \dots, 0
          \end{align}
          $$
          <p>Total of $\sim 3n$ operations only.</p>
        </section>

        <section>
          <h2>Tri-diagonal Matrices</h2>
          <p>Notice that there is no room for pivoting. Usually there is no
            need, since tridiagonal matrices arising from real applications
            typically satisfy the <em>diagonal dominance</em> condition:</p>
          $$
          |b_i| \geq |a_i| + |c_i| \quad \text{for } i = 0, \dots, n-1
          $$
          <p>which guarantees that the matrix is invertible.</p>
        </section>

        <section>
          <h2>Tri-diagonal Matrices</h2>
          <p>Do not use a general LU solver for tridiagonal matrices, since it
          will spend most of the time working on 0s.</p>
          <p class="fragment">You can typically implement a tridiagonal solver within 10 lines of
          code, so it's often practical to embed it into your program when you
          need it.</p>
          <p class="fragment">In general, one can use the LU form of the
          tridiagonal matrix $\mathbf{A}$ to compute the matrix product
          $\mathbf{A}^{-1}\mathbf{B}$, solving each column as a linear system.
          This is both faster and more accurate than finding the inverse.</p>
          <p class="fragment">If you really want to find the inverse
            $\mathbf{A}^{-1}$ explicitly, there is a general formula derived by <a href="https://www.sciencedirect.com/science/article/pii/0898122194900663?ref=cra_js_challenge&fr=RR-1">Usmami (1994)</a>. Note
            that the inverse is no longer sparse.</p>
        </section>

      </div>
    </div>
    <!-- <script src="js/head.min.js"></script> -->
    <!-- <script>
         head.js(
         "vendor/jquery.min.js",
         // "vendor/three.js/build/three.min.js",
         // "vendor/three.js/examples/js/controls/OrbitControls.js",
         /* "vendor/socket.io-client/dist/socket.io.js", */
         // "vendor/THREE.MeshLine.js",
         /* "vendor/dat.gui.min.js",*/
         "vendor/EventEmitter.js",
         // "js/samples.js",
         );
         </script> -->
    <script src="../dist/reveal.js"></script>
    <script src="../plugin/math/math.js"></script>
    <script src="../plugin/notes/notes.js"></script>
    <script src="../plugin/markdown/markdown.js"></script>
    <script src="../plugin/highlight/highlight.js"></script>

    <script>
      // More info about initialization & config:
      // - https://revealjs.com/initialization/
      // - https://revealjs.com/config/
      Reveal.initialize({
        transition: "fade",
        slideNumber: true,
        history: true,
        center: true,
        width: "100%",
        height: "100%",
        disableLayout: false,
        navigationMode: "default",
        // minScale: 1,
        // maxScale: 1,
        margin: 0.2,
        padding: 0,
        hash: true,
        math: {
          mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          // pass other options into `MathJax.Hub.Config()`
          TeX: { Macros: { RR: "{\\bf R}" } }
        },
        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath ]
      });
      Reveal.configure({
        keyboard: {
          81: null, // Do not respond to Q
          // 32: null, // Do not respond to space
        }
      });
      // Reveal.addEventListener("slidechanged", function(event) {
      //   var foot1 = document.getElementById("footer1");
      //   var foot2 = document.getElementById("footer2");
      //
      //   if (Reveal.getIndices().h === 0) {
      //     foot1.style.display = "none";
      //     foot2.style.display = "none";
      //   } else {
      //     foot1.style.display = "block";
      //     foot2.style.display = "block";
      //   }
      // });
      //
    </script>
  </body>
</html>
