<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content=
      "width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Computational Physics Lecture 33</title>
    <link rel="stylesheet" href="../dist/reset.css">
    <link rel="stylesheet" href="../dist/reveal.css">
    <link rel="stylesheet" href="../dist/theme/serif.css"
      id="theme">
    <!-- <link rel="stylesheet" href="./dist/theme/night.css" id="theme"> -->
    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href=
      "../plugin/highlight/monokai.css" id="highlight-theme">
    <link rel="stylesheet" href=
      "../css/style.css">
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <h2>Monte Carlo Integration</h2>
        </section>

        <section>
          <h2>Importance Sampling</h2>
          <p>Last time we discussed the importance sampling technique. We choose
          a weight function $w(\mathbf{x})$ to reduce the variance of the integrated function $f$.</p>
          <p class="fragment">
            If we want to evaluate the integral of function $f$ over the region $S$:
            $$
            I = \int_S f(\mathbf{x}) \mathrm{d}\mathbf{x},
            $$
            we can use the importance sampling technique to rewrite the integral as
            $$
            I = \int_S \frac{f(\mathbf{x})}{w(\mathbf{x})} w(\mathbf{x}) \mathrm{d}\mathbf{x}.
            $$
          </p>
        </section>

        <section>
          <h2>Importance Sampling</h2>
          <p>
            Effectively, we are computing the weighted average of $f/w$ over the
            region $S$. The variance of the weighted average is:
            $$
            \langle \varepsilon\rangle = \frac{\sqrt{N\times \mathrm{Var}(f/w)}}{N}\int_S w(\mathbf{x}) d\mathbf{x}
            $$
          </p>
          <p class="fragment">
            The numerator is because the variance of $N$ independent samples is $N$ times
            the variance of each sample. The denominator is because we are computing
            the weighted average of $N$ samples.
          </p>
        </section>

        <section>
          <h2>Importance Sampling</h2>
          <p>How to choose the weight function $w(\mathbf{x})$?</p>
          <p class="fragment">
            We want to choose $w$ so that it is close to $f$, but relatively
            easy to sample from.
          </p>
        </section>

        <section>
          <h2>Importance Sampling</h2>
          <p>Consider the following integral:</p>
          $$
          I = \int_0^1 x^{-1/2}e^{-x}\,\mathrm{d}x
          $$
          <p>Which part of the integrand should be absorbed into a weight function?</p>
        </section>

        <section>
          <h2>Importance Sampling</h2>
          <p>If we choose $w(x) = x^{-1/2}$, then the corresponding probability distribution is:
          $$
          p(x) = \frac{w(x)}{\int_0^1 w(x)\,\mathrm{d}x} = \frac{1}{2\sqrt{x}}
          $$</p>
          <p class="fragment">This can be sampled using the inverse function method:
          $$
            F(x) = \int_0^x p(x')\,\mathrm{d}x' = \sqrt{x},\quad x = F^{-1}(u) = u^2
          $$
            where $u$ is chosen from a uniform distribution between $0$ and $1$.
          </p>
        </section>

        <section>
          <h2>Importance Sampling</h2>
          <p>To evaluate the original integral, we draw $N$ samples $\{x_i\}$
          from the distribution $p(x)$, and compute:
            $$
            \begin{align}
            I &= \frac{1}{N}\sum_{i=1}^N \frac{f(x_i)}{w(x_i)} \int_0^1 w(x)\,\mathrm{d}x \\
            &= \frac{2}{N}\sum_{i=1}^N e^{-x_i}
            \end{align}
            $$
          </p>
        </section>

        <section>
          <h2>Importance Sampling</h2>
          <p>Here is a comparison with 100,000 samples, evaluated 1000 times:</p>
          <img src="importance-sampling.png" width="55%">
        </section>

        <section>
          <h2>Importance Sampling</h2>
          <p>Another example is to compute the likelihood of some extremely rare
          events. Let $N(x)$ be the standard normal distribution, and I would like to know the
          probability $P(x > 4)$.</p>
          <p class="fragment">
            Normally we would need to draw a huge number of samples to get a
            reasonable number of events above $4$ standard deviations. However,
            we can use importance sampling to reduce the number of samples required.
          </p>
        </section>

        <section>
          <h2>Importance Sampling</h2>
          <p>Effectively, we want to evaluate the integral:
          $$
            P(x > 4) = \int_4^\infty N(x)\,\mathrm{d}x,\quad \text{where }N(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}
          $$</p>
          <p class="fragment">
            The result is $P(x > 4) \approx 3.167\times 10^{-5}$. But even
            1,000,000 samples only gets us order unity error.
          </p>
        </section>

        <section>
          <h2>Importance Sampling</h2>
          <p>Result from sampling the normal distribution with
          1,000,000 points, repeated 1000 times:</p>
          <img src="normal-distribution.png" width="50%">
        </section>

        <section>
          <h2>Importance Sampling</h2>
          <p>Consider using the following weight function:</p>
          $$
          w(x) = \begin{cases}
          e^{-(x - 4)}, & x \geq 4 \\
          0, & x < 4
          \end{cases}
          $$
          <p class="fragment">
            This can also be sampled using the inverse function method:
            $$
            F(x) = \int_4^x w(x')\,\mathrm{d}x' = 1 - e^{4-x},\quad x = F^{-1}(u) = 4 - \log(1 - u)
            $$
          </p>
        </section>

        <section>
          <h2>Importance Sampling</h2>
          <p>We sample $N$ points $\{x_i\}$ from the distribution $w(x)$, then compute:</p>
          $$
          \begin{align}
          P(x > 4) &= \frac{1}{N}\sum_{i=1}^N \frac{N(x_i)}{w(x_i)} \int_4^\infty w(x)\,\mathrm{d}x \\
          &= \frac{1}{N}\sum_{i=1}^N \frac{1}{\sqrt{2\pi}}e^{-x_i^2/2 + x_i - 4}
          \end{align}
          $$
        </section>

        <section>
          <h2>Importance Sampling</h2>
          <p>Result from 1,000,000 samples, repeated 1000 times:</p>
          <img src="importance-sampling-2.png" width="50%">
        </section>

        <section>
          <h2>Importance Sampling</h2>
          <p>Result from 1,000,000 samples, repeated 1000 times:</p>
          <img src="importance-sampling-3.png" width="50%">
        </section>

        <section>
          <h2>Statistical Mechanics</h2>
          <p>In statistical mechanics, a physical system in thermal equilibrium
          with temperature $T$ can occupy a series of states with energies $E_i$. The
          probability for the system to be in any particular state is given by
          the Boltzmann distribution:
          $$
            P(E_i) = \frac{e^{-E_i/k_BT}}{Z}
          $$</p>
          <p class="fragment">
            where $k_B$ is the Boltzmann constant, and $Z$ acts like a
            normalization constant, and is called the <em>partition function</em> of
            the system:
            $$
            Z = \sum_i e^{-E_i/k_BT}
            $$
          </p>
        </section>

        <section>
          <h2>Statistical Mechanics</h2>
          <p>A very fundamental question in statistical mechanics is to find the
          expectation value of a physical quantity $X$. For example, this
          quantity can just be the energy:
          $$
          \langle E \rangle = \sum_i E_i P(E_i) = \frac{1}{Z}\sum_i E_i e^{-E_i/k_BT}
          $$
          </p>
          <p class="fragment">
            The physical quantity can be other things, such as the magnetization
            of a ferromagnet, or the number of particles in a particular state, etc.
          </p>
        </section>

        <section>
          <h2>Statistical Mechanics</h2>
          <p>In general, the partition function of a system is very difficult to
          compute. As a result, evaluating the expectation value of a physical
          quantity is usually nontrivial.</p>
          <p class="fragment">Monte Carlo methods are often used to evaluate
          this expectation value, and importance sampling has a very intuitive
          physical meaning.</p>
        </section>

        <section>
          <h2>Statistical Mechanics</h2>
          <p>Consider again the expectation value of a physical quantity $X$
          with respect to the Boltzmann distribution:
          $$
            \langle X \rangle = \sum_i X_i P(E_i) = \frac{1}{Z}\sum_i X_i e^{-E_i/k_BT}
          $$
          </p>
          <p class="fragment">
            If we choose a weight function $w_i$, what should be the best choice?
          </p>
        </section>

        <section>
          <h2>Statistical Mechanics</h2>
          <p>Consider the following choice of weight function:
            $$
            w_i = \frac{e^{-E_i/k_BT}}{Z}
            $$
          </p>
          <p class="fragment">
            The corresponding probability distribution is simply itself. The expectation
            value of $X$ is then:
            $$
            \langle X \rangle = \frac{1}{N}\sum_{i=1}^N \frac{X_iP(E_i)}{w_i} \sum_k w_k = \frac{1}{N}\sum_{i=1}^N X_i
            $$
          </p>
        </section>

        <section>
          <h2>Statistical Mechanics</h2>
          <p>Therefore, the Monte Carlo method of evaluating the expectation
          value of any variable in a statistical system in thermal equilibrium is very simple:</p>
          <p class="fragment">
            Choose $N$ states according the Boltzmann distribution, and compute the
            average of the variable over these states:
            $$
            \langle X \rangle = \frac{1}{N}\sum_{i=1}^N X_i
            $$
          </p>
        </section>

        <section>
          <h2>Statistical Mechanics</h2>
          <p>For example, the energy of the system is given by:
          $$
             E = \frac{1}{N}\sum_i E_i
          $$
          where $E_i$ is drawn from the exponential distribution $P(E_i)$:
          $$
          P(E_i) \propto e^{-E_i/k_BT}
          $$
          </p>
        </section>

      </div>
    </div>
    <!-- <script src="js/head.min.js"></script> -->
    <!-- <script>
         head.js(
         "vendor/jquery.min.js",
         // "vendor/three.js/build/three.min.js",
         // "vendor/three.js/examples/js/controls/OrbitControls.js",
         /* "vendor/socket.io-client/dist/socket.io.js", */
         // "vendor/THREE.MeshLine.js",
         /* "vendor/dat.gui.min.js",*/
         "vendor/EventEmitter.js",
         // "js/samples.js",
         );
         </script> -->
    <script src="../dist/reveal.js"></script>
    <script src="../plugin/math/math.js"></script>
    <script src="../plugin/notes/notes.js"></script>
    <script src="../plugin/markdown/markdown.js"></script>
    <script src="../plugin/highlight/highlight.js"></script>

    <script>
      // More info about initialization & config:
      // - https://revealjs.com/initialization/
      // - https://revealjs.com/config/
      Reveal.initialize({
        transition: "fade",
        slideNumber: true,
        history: true,
        center: true,
        width: "100%",
        height: "100%",
        disableLayout: false,
        navigationMode: "default",
        // minScale: 1,
        // maxScale: 1,
        margin: 0.2,
        padding: 0,
        hash: true,
        math: {
          mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          // pass other options into `MathJax.Hub.Config()`
          TeX: { Macros: { RR: "{\\bf R}" } }
        },
        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath ]
      });
      Reveal.configure({
        keyboard: {
          81: null, // Do not respond to Q
          // 32: null, // Do not respond to space
        }
      });
      // Reveal.addEventListener("slidechanged", function(event) {
      //   var foot1 = document.getElementById("footer1");
      //   var foot2 = document.getElementById("footer2");
      //
      //   if (Reveal.getIndices().h === 0) {
      //     foot1.style.display = "none";
      //     foot2.style.display = "none";
      //   } else {
      //     foot1.style.display = "block";
      //     foot2.style.display = "block";
      //   }
      // });
      //
    </script>
  </body>
</html>
