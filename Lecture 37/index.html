<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content=
      "width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Computational Physics Lecture 37</title>
    <link rel="stylesheet" href="../dist/reset.css">
    <link rel="stylesheet" href="../dist/reveal.css">
    <link rel="stylesheet" href="../dist/theme/serif.css"
      id="theme">
    <!-- <link rel="stylesheet" href="./dist/theme/night.css" id="theme"> -->
    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href=
      "../plugin/highlight/monokai.css" id="highlight-theme">
    <link rel="stylesheet" href=
      "../css/style.css">
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <h2>Bayesian Inference</h2>
        </section>

        <section>
          <h2>Bayesian Inference</h2>
          <p>Last time, we discussed how MCMC is used in Bayesian inference to
          compute the posterior probability distribution:
          $$
            P(\theta|x) \propto P(x|\theta)P(\theta)
          $$</p>
          <p class="fragment">
            $x$ represents the data, $\theta$ represents the parameters of the
            model, $P(x|\theta)$ is the likelihood of data $x$ given the model
            parameters $\theta$, and $P(\theta)$ is the prior.
          </p>
        </section>

        <section>
          <h2>Bayesian Inference</h2>
          <p>Let's consider an example. Say we have a model of correlation between people's weight and height:
            $$
            W_i = \alpha + \beta H_i + \epsilon_i
            $$
            where $\epsilon_i \sim N(0, \sigma^2)$ is a normally distributed random variable.
          </p>
          <p class="fragment">The parameters of this model are $\alpha$, $\beta$, and $\sigma$.</p>
        </section>

        <!-- <section>
             <h2>Bayesian Inference</h2>
             <p>The goal is to compute the posterior distribution $P(\alpha, \beta, \sigma|\mathbf{x})$ given
             some data vector $\mathbf{W}$ and $\mathbf{H}$:
             $$
             P(\alpha, \beta, \sigma|\mathbf{W}, \mathbf{H}) \propto \prod_{i=1}^N P(W_i, H_i|\alpha, \beta, \sigma)P(\alpha)P(\beta)P(\sigma)
             $$</p>
             </section>
        -->
        <section>
          <h2>Bayesian Inference</h2>
          <p>Assuming we took this set of data data $\{H_i, W_i\}$:</p>
          <img src="weight_vs_height_random.png" width="45%">
          <p class="fragment">The goal is to infer the parameters $\alpha$, $\beta$, and $\sigma$.</p>
        </section>

        <section>
          <h2>Bayesian Inference</h2>
          <p>Let's assume that the prior distributions are:
          $$
            \begin{align}
              P(\alpha) &= N(0, 10^2) \\
              P(\beta) &= N(0, 10^2) \\
              P(\sigma) &= 0.1 \quad(\text{uniform with } 0 <\sigma < 10)
            \end{align}
          $$</p>
          <p class="fragment">The posterior distribution is then:
            $$
              P(\alpha, \beta, \sigma|\mathbf{W}, \mathbf{H}) \propto \left[\prod_{i=1}^N P(W_i, H_i|\alpha, \beta, \sigma)\right]P(\alpha)P(\beta)P(\sigma)
            $$
          </p>
        </section>

        <section>
          <h2>Bayesian Inference</h2>
          <p>How to compute the probability $P(W_i, H_i|\alpha, \beta, \sigma)$?</p>
          <p class="fragment">Under the model assumption, the data is normally distributed with mean $\alpha + \beta H_i$ and standard deviation $\sigma$:
          $$
            P(W_i, H_i|\alpha, \beta, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(W_i - \alpha - \beta H_i)^2}{2\sigma^2}\right)
          $$</p>
        </section>

        <section>
          <h2>Bayesian Inference</h2>
          <p>Normal Metropolis-Hastings would require us to start from some parameter vector $(\alpha_0, \beta_0, \gamma_0)$,
          and come up with a proposal $g$ that generates the next triplet $(\alpha_1, \beta_1, \gamma_1)$.
          Then we accept this new triplet according to the standard acceptance probability.</p>
          <p class="fragment">However, in higher dimensions, constructing a
          proposal distribution may be non-trivial, and acceptance rate can be
          rather low.</p>
        </section>

        <section>
          <h2>Gibbs Sampling</h2>
          <p>The Gibbs sampling technique draws random samples one dimension at a time. So for our example, we sample the
          three parameters one by one:
          $$
            \begin{align}
              \alpha_{n+1} &\sim P(\alpha|\beta_n, \sigma_n, \mathbf{W}, \mathbf{H}) \\
              \beta_{n+1} &\sim P(\beta|\alpha_{n+1}, \sigma_n, \mathbf{W}, \mathbf{H}) \\
              \sigma_{n+1} &\sim P(\sigma|\alpha_{n+1}, \beta_{n+1}, \mathbf{W}, \mathbf{H})
            \end{align}
          $$</p>
        </section>

        <section>
          <h2>Gibbs Sampling</h2>
          <p>For example, the distribution for $\alpha$ alone can be written as:
          $$
            \begin{align}
              P(\alpha|\beta_n, \sigma_n, \mathbf{W}, \mathbf{H}) &\propto P(\alpha, \beta_n, \sigma_n|\mathbf{W}, \mathbf{H}) \\
              &\propto \left[\prod_{i=1}^N P(W_i, H_i|\alpha, \beta_n, \sigma_n)\right]P(\alpha)P(\beta_n)P(\sigma_n) \\
              &\propto \left[\prod_{i=1}^N \frac{1}{\sqrt{2\pi\sigma_n^2}}\exp\left(-\frac{(W_i - \alpha_n - \beta_n H_i)^2}{2\sigma_n^2}\right)\right]P(\alpha)
            \end{align}
          $$</p>
        </section>

        <section>
          <h2>Gibbs Sampling</h2>
          <p>In this particular example, the distribution for $\alpha$ is a
          normal distribution (times the prior), and we can write down its mean and variance:
          $$
            P(\alpha|\beta_n, \sigma_n, \mathbf{W}, \mathbf{H}) = \mathcal{N}\left(\frac{\sum_{i=1}^N(W_i - \beta_n H_i)}{N}, \frac{\sigma_n^2}{N}\right)P(\alpha)
          $$</p>
          <p class="fragment">In more complex cases, we will still need to use the 1D
          Metropolis algorithm to sample this distribution.</p>
        </section>

        <section>
          <h2>Gibbs Sampling</h2>
          <p>Similarly, we can write down the distributions for $\beta$ and $\sigma$:
          $$
            \begin{align}
              P(\beta|\alpha_{n+1}, \sigma_n, \mathbf{W}, \mathbf{H}) &= \mathcal{N}\left(\frac{\sum_{i=1}^N(W_i - \alpha_{n+1})H_i}{\sum_{i=1}^NH_i^2}, \frac{\sigma_n^2}{\sum_{i=1}^NH_i^2}\right)P(\beta) \\
              P(1/\sigma^2|\alpha_{n+1}, \beta_{n+1}, \mathbf{W}, \mathbf{H}) &= \text{Gamma}\left(\frac{N}{2}, \frac{\sum_{i=1}^N(W_i - \alpha_{n+1} - \beta_{n+1}H_i)^2}{2}\right)P(1/\sigma^2)
            \end{align}
          $$</p>
          <p class="fragment">
             The Gamma distribution is defined as:
              $$
                \text{Gamma}(k, \theta) = \frac{1}{\Gamma(k)\theta^k}x^{k-1}e^{-x/\theta}
              $$
          </p>
        </section>

        <section>
          <h2>Pair Plot</h2>
          <p>The result of carrying out the full Gibbs sampling algorithm on our example can be plotted as:</p>
          <img src="pair_plot.png" width="45%">
        </section>

        <section>
          <h2>Pair Plot</h2>
          <p>This is called a pair plot. Diagonal plots are simply the histograms of the individual parameters, and off-diagonal plots represent
          the cross-correlation between a pair of parameters.</p>
           <p class="fragment">Another way to think of this is that it
          visualizes the high-dimensional parameter space one 2D slice at a
          time.</p>
        </section>

        <section>
          <h2>Pair Plot</h2>
          <img src="pair_plot_pulsar.png" width="50%">
        </section>

        <section>
          <h2>Bayesian Inference</h2>
          <p>Nowadays there are many MCMC packages for Bayesian inference. For example, <a href="https://www.pymc.io/welcome.html">PyMC</a> is
          a Python package that encapsulates the whole Bayesian modelling process using MCMC methods.</p>
        </section>

        <section>
          <h2>Monte Carlo Methods</h2>
          <p>Over the past 3 weeks or so, we have discussed a wide range of Monte Carlo methods:</p>
          <ul>
            <li class="fragment">Monte Carlo methods that simulate stochastic processes such as Brownian motion.</li>
            <li class="fragment">Monte Carlo integration and importance sampling.</li>
            <li class="fragment">Markov Chain Monte Carlo, which can be used to sample from arbitrary distributions.</li>
            <li class="fragment">MCMC for simulating statistical physics.</li>
            <li class="fragment">MCMC for optimization problems.</li>
            <li class="fragment">MCMC for Bayesian inference.</li>
          </ul>
        </section>


      </div>
    </div>
    <!-- <script src="js/head.min.js"></script> -->
    <!-- <script>
         head.js(
         "vendor/jquery.min.js",
         // "vendor/three.js/build/three.min.js",
         // "vendor/three.js/examples/js/controls/OrbitControls.js",
         /* "vendor/socket.io-client/dist/socket.io.js", */
         // "vendor/THREE.MeshLine.js",
         /* "vendor/dat.gui.min.js",*/
         "vendor/EventEmitter.js",
         // "js/samples.js",
         );
         </script> -->
    <script src="../dist/reveal.js"></script>
    <script src="../plugin/math/math.js"></script>
    <script src="../plugin/notes/notes.js"></script>
    <script src="../plugin/markdown/markdown.js"></script>
    <script src="../plugin/highlight/highlight.js"></script>

    <script>
      // More info about initialization & config:
      // - https://revealjs.com/initialization/
      // - https://revealjs.com/config/
      Reveal.initialize({
        transition: "fade",
        slideNumber: true,
        history: true,
        center: true,
        width: "100%",
        height: "100%",
        disableLayout: false,
        navigationMode: "default",
        // minScale: 1,
        // maxScale: 1,
        margin: 0.2,
        padding: 0,
        hash: true,
        math: {
          mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          // pass other options into `MathJax.Hub.Config()`
          TeX: { Macros: { RR: "{\\bf R}" } }
        },
        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath ]
      });
      Reveal.configure({
        keyboard: {
          81: null, // Do not respond to Q
          // 32: null, // Do not respond to space
        }
      });
      // Reveal.addEventListener("slidechanged", function(event) {
      //   var foot1 = document.getElementById("footer1");
      //   var foot2 = document.getElementById("footer2");
      //
      //   if (Reveal.getIndices().h === 0) {
      //     foot1.style.display = "none";
      //     foot2.style.display = "none";
      //   } else {
      //     foot1.style.display = "block";
      //     foot2.style.display = "block";
      //   }
      // });
      //
    </script>
  </body>
</html>
