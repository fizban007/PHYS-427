<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content=
      "width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Computational Physics Lecture 36</title>
    <link rel="stylesheet" href="../dist/reset.css">
    <link rel="stylesheet" href="../dist/reveal.css">
    <link rel="stylesheet" href="../dist/theme/serif.css"
      id="theme">
    <!-- <link rel="stylesheet" href="./dist/theme/night.css" id="theme"> -->
    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href=
      "../plugin/highlight/monokai.css" id="highlight-theme">
    <link rel="stylesheet" href=
      "../css/style.css">
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <h2>Bayesian Inference</h2>
        </section>

        <section>
          <h2>Bayes Theorem</h2>
          <p>Bayesian inference is one of the major use cases for MCMC. I hope by the end
          of this lecture you will understand why.</p>
          <p class="fragment">The basis of Bayesian inference is Bayes theorem. For two events $A$ and $B$, Bayes theorem states that
            the probability of event $A$ <em>given</em> the event $B$ is:
          $$
          P(A|B) = \frac{P(B|A)P(A)}{P(B)}
          $$</p>
          <p class="fragment">
            The conditional probability $P(A|B)$ is defined as $P(A|B) = P(A\cap B)/P(B)$.
          </p>
        </section>

        <section>
          <h2>Bayes Theorem</h2>
          <p>Bayes theorem seems to be stating something obvious, but it can be
          often used to arrive at conclusions not at all trivial.</p>
          <p class="fragment">
            Let's consider an example. Suppose we have a test for a disease
            that is 99% accurate. That is, if you have the disease, the test will
            return a positive result 99% of the time. If you don't have the disease,
            the test will return a negative result 99% of the time. Suppose the
            disease is rare, and only 1 in 1000 people have it. If you take the test
            and it returns a positive result, what is the probability that you have
            the disease?
          </p>
        </section>

        <section>
          <h2>Bayes Theorem</h2>
          <p>The probability of you having the disease with a positive result is only 9%!:
          $$
            P(D|+) = \frac{P(+|D)P(D)}{P(+)} = \frac{0.99\times 0.001}{0.99\times 0.001 + 0.01\times 0.999} = 0.09
          $$</p>
          <p class="fragment">
            The main reason for this result is that it is very unlikely that you
            have the disease in the first place. Since the test returns a false
            positive 1% of the time, the number of false positives tends to be
            much larger than the number of true positives.
          </p>
        </section>

        <section>
          <h2>Bayes Theorem</h2>
          <p>The probability of having the disease in general $P(D)$ is called the <em>prior probability</em>. It
          reflects our current beliefs about the general probability of an event happening.</p>
          <p class="fragment">
            The conditional probability of having the disease, given the positive test result $P(D|+)$, is called the <em>posterior
            probability</em>. It reflects our adjusted belief given the data at hand.
          </p>
        </section>

        <section>
          <h2>Bayesian Inference</h2>
          <p>In science, Bayes theorem is often use to answer the question: What
          is the most likely model parameters given some data we observe?</p>
          <p class="fragment">A "Model" can be any mathematical prescription
          describing nature, with certain parameters. For example, in Brownian
          motion, the drift $\mu$
          and diffusion coefficient $D$ are parameters of the model.</p>
        </section>

        <section>
          <h2>Bayesian Inference</h2>
          <p>In Bayesian inference, there is a fundamental distinction between:</p>
          <ul>
            <li>Observable quantities $x$, which is the data we collect.</li>
            <li class="fragment">Unknown parameters $\theta$, which is what we want to infer.</li>
          </ul>
          <p class="fragment">$\theta$ can represent model parameters, missing
          data, hidden variables, etc. But these parameters are treated as
          random variables with their own distributions</p>
        </section>

        <section>
          <h2>Bayesian Inference</h2>
          <p>A model can be thought of as $P(x|\theta)$, the probability of
          getting data $x$ given the parameters $\theta$. Such a model often
          accounts for uncertainties in obtaining data $x$. $P(x|\theta)$ is
          often called the "likelihood".</p>
          <p class="fragment">However, in the Bayesian view, $\theta$ is
          unknown, and we as model builders need to specify a "prior
          distribution" $P(\theta)$ that describes this uncertainty.</p>
        </section>

        <section>
          <h2>Bayesian Inference</h2>
          <p>Bayes Theorem now automatically gives us the posterior probability
          for any model parameter given some observed data:
          $$
          P(\theta|x) = \frac{P(x|\theta)P(\theta)}{\int P(x|\theta)P(\theta)\,d\theta} \propto P(x|\theta)P(\theta)
          $$</p>
          <p class="fragment">In other words, the posterior distribution is the
          product of the prior distribution with the likelihood function.</p>
          <p class="fragment">The prior distribution describes our uncertainty about $\theta$ <em>before</em> we
            see the data. The posterior distribution describes our uncertainty <em>after</em> we see the data.</p>
        </section>

        <section>
          <h2>Bayesian Inference</h2>
          <p>Let's consider an example. My model is a normal distribution $N(0,
          \sigma^2)$ with 0 mean and some unknown variance $\sigma^2$. Therefore $\theta$ can be taken as the standard deviation $\sigma$.</p>
          <p class="fragment">I don't know much about $\sigma$, so I will
          assume a uniform prior $P(\sigma) = 0.2$ for $0\leq \sigma \leq 5$.</p>
          <p class="fragment">Now I make a measurement, $x = x_0$. With this new knowledge, what can I say about
          the parameter $\sigma$?</p>
        </section>

        <section>
          <h2>Bayesian Inference</h2>
          <p>The posterior probability for $\sigma$ is given by the Bayes theorem:
          $$
            P(\sigma|x_0) \propto P(x_0|\sigma)P(\sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-x_0^2/2\sigma^2}\times 0.2
          $$</p>
        </section>

        <section>
          <h2>Bayesian Inference</h2>
          <p>This is the posterior distribution of $\sigma$ for different $x$ values:</p>
          <img src="normal_variance.png" width="55%">
        </section>

        <section>
          <h2>Bayesian Inference</h2>
          <p>Suppose we have multiple measurements $\{x_i\}$, the likelihood function is now the product of individual likelihoods:
          $$
            P(\{x_i\}|\sigma) = \prod_i \frac{1}{\sqrt{2\pi\sigma^2}}e^{-x_i^2/2\sigma^2}
          $$</p>
          <p class="fragment">
            The posterior distribution is now:
            $$
            P(\sigma|\{x_i\}) \propto P(\{x_i\}|\sigma)P(\sigma) = \prod_i \frac{1}{\sqrt{2\pi\sigma^2}}e^{-x_i^2/2\sigma^2}\times 0.2
            $$
          </p>
        </section>

        <section>
          <h2>Bayesian Inference</h2>
          <p>For example, given a dataset $\{0.0, 0.01, 0.01, 0.02, 0.02, 0.03, 0.03, 4\}$, the posterior distribution of the
          standard deviation $\sigma$ looks like this:</p>
          <img src="normal_variance_dataset.png" width="50%">
        </section>

        <section>
          <h2>The Role of MCMC</h2>
          <p>How does Bayesian inference connect to MCMC?</p>
          <p class="fragment">
            In the earlier example, we can calculate the posterior distribution
            of $\sigma$ analytically. However, in most cases, the posterior
            distribution is not analytically tractable. In such cases, we need
            to use MCMC to sample from the posterior distribution, in order to
            compute its mean and variance.
          </p>
        </section>

        <section>
          <h2>The Role of MCMC</h2>
          <p>Consider our normal distribution example again. We want to sample directly from the posterior
          distribution:
          $$
            P(\sigma|x) \propto P(x|\sigma)P(\sigma)
          $$</p>
          <p class="fragment">
            In order to do this, we construct a Markov chain starting at a random $\sigma_0$. Then,
            we use the Metropolis-Hastings algorithm to generate the next step, until we arrive at an
            equilibrium distribution.
          </p>
        </section>

        <section>
          <h2>The Role of MCMC</h2>
          <p>Given $\sigma_0$, our proposal distribution can simply be $g(\sigma_1) = N(\sigma_0, 0.1^2)$. The acceptance
            probability is then:
          $$
            A(\sigma_1) = \min\left(1, \frac{P(\sigma_1|x)}{P(\sigma_0|x)}\right) = \min\left(1, \frac{P(x|\sigma_1)P(\sigma_1)}{P(x|\sigma_0)P(\sigma_0)}\right)
          $$</p>
          <p class="fragment">
            $P(x|\sigma_0)$ and $P(x|\sigma_1)$ mean the probability of getting data $x$ at parameter $\sigma_0$ and $\sigma_1$.
          </p>
        </section>

        <section>
          <h2>The Role of MCMC</h2>
          <p>Result of MCMC simulation for the same dataset $\{0.0, 0.01, 0.01, 0.02, 0.02, 0.03, 0.03, 4\}$:</p>
          <img src="mcmc.png" width="55%">
        </section>

        <section>
          <h2>The Role of MCMC</h2>
          <p>A crucial advantage of MCMC is that it does not need to compute the denominator in Bayes theorem:
          $$
            P(\theta|x) = \frac{P(x|\theta)P(\theta)}{\int P(x|\theta)P(\theta)\,d\theta} \propto P(x|\theta)P(\theta)
          $$</p>
          <p class="fragment">Computing $P(x|\theta)$ is "model evaluation", and
          is what this course has been all about up to now. We can now use all
          these techniques to compute the inverse problem of finding the
          probability distribution of the unknown parameters!</p>
        </section>

      </div>
    </div>
    <!-- <script src="js/head.min.js"></script> -->
    <!-- <script>
         head.js(
         "vendor/jquery.min.js",
         // "vendor/three.js/build/three.min.js",
         // "vendor/three.js/examples/js/controls/OrbitControls.js",
         /* "vendor/socket.io-client/dist/socket.io.js", */
         // "vendor/THREE.MeshLine.js",
         /* "vendor/dat.gui.min.js",*/
         "vendor/EventEmitter.js",
         // "js/samples.js",
         );
         </script> -->
    <script src="../dist/reveal.js"></script>
    <script src="../plugin/math/math.js"></script>
    <script src="../plugin/notes/notes.js"></script>
    <script src="../plugin/markdown/markdown.js"></script>
    <script src="../plugin/highlight/highlight.js"></script>

    <script>
      // More info about initialization & config:
      // - https://revealjs.com/initialization/
      // - https://revealjs.com/config/
      Reveal.initialize({
        transition: "fade",
        slideNumber: true,
        history: true,
        center: true,
        width: "100%",
        height: "100%",
        disableLayout: false,
        navigationMode: "default",
        // minScale: 1,
        // maxScale: 1,
        margin: 0.2,
        padding: 0,
        hash: true,
        math: {
          mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          // pass other options into `MathJax.Hub.Config()`
          TeX: { Macros: { RR: "{\\bf R}" } }
        },
        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath ]
      });
      Reveal.configure({
        keyboard: {
          81: null, // Do not respond to Q
          // 32: null, // Do not respond to space
        }
      });
      // Reveal.addEventListener("slidechanged", function(event) {
      //   var foot1 = document.getElementById("footer1");
      //   var foot2 = document.getElementById("footer2");
      //
      //   if (Reveal.getIndices().h === 0) {
      //     foot1.style.display = "none";
      //     foot2.style.display = "none";
      //   } else {
      //     foot1.style.display = "block";
      //     foot2.style.display = "block";
      //   }
      // });
      //
    </script>
  </body>
</html>
