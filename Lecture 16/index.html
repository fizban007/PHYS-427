<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content=
      "width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Computational Physics Lecture 16</title>
    <link rel="stylesheet" href="../dist/reset.css">
    <link rel="stylesheet" href="../dist/reveal.css">
    <link rel="stylesheet" href="../dist/theme/serif.css"
      id="theme">
    <!-- <link rel="stylesheet" href="./dist/theme/night.css" id="theme"> -->
    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href=
      "../plugin/highlight/monokai.css" id="highlight-theme">
    <link rel="stylesheet" href=
      "../css/style.css">
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <h2>Boundary Value Problems for ODEs</h2>
        </section>

        <section>
          <h2>Boundary Value Problems</h2>
          <p>A boundary value problem (BVP) is an ODE with boundary conditions</p>
          $$
          \begin{align*}
            \frac{d^2y}{dx^2} &= f(x,y,y') \\
            y(a) &= \alpha \\
            y(b) &= \beta
          \end{align*}
          $$
        </section>

        <section>
          <h2>Relaxation Method</h2>
          <p>
            Discretizing the domain $[a,b]$ into $N + 1$ points $x_i$ with
            $x_0 = a$ and $x_N = b$, we can in general rewrite the ODE as a
            <em>Finite Difference Equation</em>:
          </p>
          $$
          \frac{y_{i+1} - 2y_i + y_{i-1}}{h^2} = f(x_i,y_i,\frac{y_{i+1}-y_{i-1}}{2h})
          $$
          <p>
            where $h = (b-a)/N$ is the step size. This is a system of $N-1$ equations
            for the $N-1$ unknowns $y_1,\ldots,y_{N-1}$.
          </p>
        </section>

        <section>
          <h2>Relaxation Method</h2>
          <p>In general, this is a set of nonlinear equations. It needs to be
          solved iteratively using Newton's method.</p>
          $$
          F_i(\mathbf{y}) = \frac{y_{i+1} - 2y_i + y_{i-1}}{h^2} - f(x_i,y_i,\frac{y_{i+1}-y_{i-1}}{2h}) = 0
          $$
          <div class="fragment">
            <p>An iteration with Newton's method looks like this:</p>
            $$
            \mathbf{y}_\mathrm{new} = \mathbf{y}_\mathrm{old} - \mathbf{J}^{-1}\mathbf{F}(\mathbf{y}_\mathrm{old})
            $$
            <p>How to write down the Jacobian $\mathbf{J}$?</p>
          </div>
        </section>

        <section>
          <h2>Relaxation Method</h2>
          <p>Since we know the form of function $F_i(\mathbf{y})$, we can take
          the partial derivatives directly:</p>
          $$
          \frac{\partial F_i}{\partial y_j} =
          \begin{cases}
          \frac{1}{h^2} - \frac{\partial f}{\partial y'}\frac{1}{h},\quad j = i + 1 \\
          -\frac{2}{h^2} - \frac{\partial f}{\partial y},\quad j = i \\
          \frac{1}{h^2} + \frac{\partial f}{\partial y'}\frac{1}{h},\quad j = i - 1
          \end{cases}
          $$
          <p class="fragment">This is a <em>tri-diagonal matrix</em>, and its inverse can be
            found relatively easily. </p>
          <p class="fragment">Tri-diagonal matrices are <em>sparse</em>, meaning
          that their elements are mostly zero.</p>
        </section>

        <section>
          <h2>Relaxation Method</h2>
          <p>When there are $M$ equations, this method is much more complicated.
          This is because the Jacobian $\mathbf{J}$ now has dimension $M(N-1)\times
          M(N-1)$.</p>
          <div class="fragment">
            <p>$\mathbf{J}$ is still a sparse matrix, with a special
            block-diagonal structure. See Numerical Recipes (3rd Edition) Section
            18.3 for more details.</p>
          </div>
        </section>

        <section>
          <h2>Numerical Linear Algebra</h2>
        </section>

        <section>
          <h2>Numerical Linear Algebra</h2>
          <p>Linear algebra is an important part of a computational physicist's
          tool box. This includes:</p>
          <ul>
            <li>Solving systems of linear (matrix) equations;</li>
            <li>Computing the inverse of a matrix, and/or its determinant;</li>
            <li>Linear least-squares problem;</li>
            <li>Diagonalizing matrices, finding eigenvalues and eigenvectors;</li>
          </ul>
        </section>

        <section>
          <h2>Numerical Linear Algebra</h2>
          <p>Many numerical linear algebra algorithms have been implemented
          in BLAS and LAPACK:</p>
          <ul>
            <li>BLAS (Basic Linear Algebra Subprograms) provides basic
            matrix-vector or matrix-matrix operations.</li>
            <li>LAPACK (Linear Algebra PACKage) provides solvers of linear
            systems, eigenvalue problems, and matrix factorization algorithms.</li>
          </ul>
          <p>These libraries are written in Fortran, and are available on most
          platforms. Most languages provide wrappers for these libraries, so it
          doesn't make sense writing one's own.</p>
        </section>

        <section>
          <h2>Numerical Linear Algebra</h2>
          <p>For C++, there is a great modern library called <em>Eigen</em>.</p>
          <a href="https://eigen.tuxfamily.org/index.php">Eigen Homepage</a>
          <p>The Matrix and Vector classes are native C++ constructs, and far
          easier to use than a Fortran library.</p>
        </section>

        <section>
          <h2>Numerical Linear Algebra</h2>
          <p>For Python, <code>numpy.linalg</code> and <code>scipy.linalg</code> provide
          linear algebra functionality similar to BLAS and LAPACK.</p>
          <p class="fragment">Our goal for the next few lectures is to provide
          you with a map for navigating these libraries, explaining what each
          algorithm does, so that you have an idea what to use when you need them.</p>
        </section>

        <section>
          <h2>Solving Linear Systems</h2>
          <p>A linear system of equations is a set of equations of the form</p>
          $$
          \begin{align*}
            a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &= b_1 \\
            a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &= b_2 \\
            \vdots &= \vdots \\
            a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n &= b_n
          \end{align*}
          $$
          <p>where $a_{ij}$ and $b_i$ are known numbers, and $x_i$ are unknowns.</p>
        </section>

        <section>
          <h2>Solving Linear Systems</h2>
          <p>Linear systems can be written in matrix form:</p>
          $$
          \mathbf{A}\mathbf{x} = \mathbf{b}
          $$
          <p>where $\mathbf{A}$ is an $m\times n$ matrix, and both $\mathbf{x}$
          and $\mathbf{b}$ are $n$-dimensional vectors.</p>
          <div class="fragment">
            <p>If $m > n$, the system is over-determined and there may not be a
            solution. If $m < n$, the system is under-determined and will in
            general have infinite solutions.</p>
          </div>
        </section>

        <section>
          <h2>Solving Linear Systems</h2>
          <p>Even when square matrices $m = n$, the system may be
          under-determined if the matrix is singular. This happens if one or
          more of the equations can be written as a linear combination of other
          equations.</p>
          <p class="fragment">When the system is under-determined, finding the <em>solution space</em> involves
          finding the Singular Value Decomposition (SVD) of the matrix</p>
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>One of the most basic algorithms for solving linear systems is
          Gaussian Elimination with backsubstitution.</p>
          <div class="fragment">
            <p>Consider the following matrix equation:</p>
            $$
            \begin{pmatrix}
              1 & 2 & 3 \\
              4 & 6 & 7 \\
              5 & 8 & 9
            \end{pmatrix}
            \begin{pmatrix}
              x_1 \\
              x_2 \\
              x_3
            \end{pmatrix}
            =
            \begin{pmatrix}
              1 \\
              2 \\
              4
            \end{pmatrix}
            $$
            <p class="fragment">
              We can eliminate the first column by subtracting the first row
              from the second and third rows
            </p>
          </div>
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>After the first step, the matrix equation becomes:</p>
          $$
          \begin{pmatrix}
            1 & 2 & 3 \\
            0 & -2 & -5 \\
            0 & -2 & -6
          \end{pmatrix}
          \begin{pmatrix}
            x_1 \\
            x_2 \\
            x_3
          \end{pmatrix}
          =
          \begin{pmatrix}
            1 \\
            -2 \\
            -1
          \end{pmatrix}
          $$
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>Next, we eliminate the second column by subtracting the second row
          from the third row:</p>
          $$
          \begin{pmatrix}
            1 & 2 & 3 \\
            0 & -2 & -5 \\
            0 & 0 & -1
          \end{pmatrix}
          \begin{pmatrix}
            x_1 \\
            x_2 \\
            x_3
          \end{pmatrix}
          =
          \begin{pmatrix}
            1 \\
            -2 \\
            1
          \end{pmatrix}
          $$
          <p class="fragment">Now our matrix is in an upper-triangular form. We
          are ready for backsubstitution.</p>
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>First we can solve the last equation for $x_3$:</p>
          $$
          -x_3 = 1 \implies x_3 = -1
          $$
          <div class="fragment">
            <p>Then we can substitute this into the second equation to solve for $x_2$:</p>
            $$
            -2x_2 - 5(-1) = -2 \implies x_2 = 3.5
            $$
            <div class="fragment">
              <p>Finally, we substitute the above into the first equation to solve for $x_1$:</p>
              $$
              x_1 + 2(3.5) + 3(-1) = 1 \implies x_1 = -3
              $$
            </div>
          </div>
        </section>

        <section>
          <h2>Pivoting</h2>
          <p>What happens when the matrix looks like this:</p>
          $$
          \begin{pmatrix}
            0 & 2 & 3 \\
            4 & 6 & 7 \\
            5 & 8 & 9
          \end{pmatrix}
          \begin{pmatrix}
            x_1 \\
            x_2 \\
            x_3
          \end{pmatrix}
          =
          \begin{pmatrix}
            1 \\
            2 \\
            4
          \end{pmatrix}
          $$
          <p class="fragment">Subtracting the first row from the second does not
          eliminate the first element of the second row.</p>
        </section>

        <section>
          <h2>Pivoting</h2>
          <p>We can fix this by swapping the first and the third row (due to
          partial pivoting rule):</p>
          $$
          \begin{pmatrix}
            5 & 8 & 9 \\
            4 & 6 & 7 \\
            0 & 2 & 3
          \end{pmatrix}
          \begin{pmatrix}
            x_1 \\
            x_2 \\
            x_3
          \end{pmatrix}
          =
          \begin{pmatrix}
            4 \\
            2 \\
            1
          \end{pmatrix}
          $$
          <p>Note that you will need to swap the corresponding elements of the
          right hand side too! This procedure is equivalent to rearranging the order of the equations,
          which does not affect the solution.</p>
        </section>

        <section>
          <h2>Pivoting</h2>
          <p>Exchanging rows only is called <em>partial pivoting</em>, while
          exchanging both rows and columns is called <em>full pivoting</em>.</p>
          <p class="fragment">When exchanging rows, we need to swap the right
          hand side; when exchanging columns, we need to exchange the order of
          the unknown variables.</p>
          <p class="fragment">A way to do partial pivoting systematically is to always move
          the row with the <em>largest</em> diagonal element up to the current row.</p>
        </section>


      </div>
    </div>
    <!-- <script src="js/head.min.js"></script> -->
    <!-- <script>
         head.js(
         "vendor/jquery.min.js",
         // "vendor/three.js/build/three.min.js",
         // "vendor/three.js/examples/js/controls/OrbitControls.js",
         /* "vendor/socket.io-client/dist/socket.io.js", */
         // "vendor/THREE.MeshLine.js",
         /* "vendor/dat.gui.min.js",*/
         "vendor/EventEmitter.js",
         // "js/samples.js",
         );
         </script> -->
    <script src="../dist/reveal.js"></script>
    <script src="../plugin/math/math.js"></script>
    <script src="../plugin/notes/notes.js"></script>
    <script src="../plugin/markdown/markdown.js"></script>
    <script src="../plugin/highlight/highlight.js"></script>

    <script>
      // More info about initialization & config:
      // - https://revealjs.com/initialization/
      // - https://revealjs.com/config/
      Reveal.initialize({
        transition: "fade",
        slideNumber: true,
        history: true,
        center: true,
        width: "100%",
        height: "100%",
        disableLayout: false,
        navigationMode: "default",
        // minScale: 1,
        // maxScale: 1,
        margin: 0.2,
        padding: 0,
        hash: true,
        math: {
          mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          // pass other options into `MathJax.Hub.Config()`
          TeX: { Macros: { RR: "{\\bf R}" } }
        },
        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath ]
      });
      Reveal.configure({
        keyboard: {
          81: null, // Do not respond to Q
          // 32: null, // Do not respond to space
        }
      });
      // Reveal.addEventListener("slidechanged", function(event) {
      //   var foot1 = document.getElementById("footer1");
      //   var foot2 = document.getElementById("footer2");
      //
      //   if (Reveal.getIndices().h === 0) {
      //     foot1.style.display = "none";
      //     foot2.style.display = "none";
      //   } else {
      //     foot1.style.display = "block";
      //     foot2.style.display = "block";
      //   }
      // });
      //
    </script>
  </body>
</html>
