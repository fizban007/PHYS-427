<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content=
      "width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Computational Physics Lecture 33</title>
    <link rel="stylesheet" href="../dist/reset.css">
    <link rel="stylesheet" href="../dist/reveal.css">
    <link rel="stylesheet" href="../dist/theme/serif.css"
      id="theme">
    <!-- <link rel="stylesheet" href="./dist/theme/night.css" id="theme"> -->
    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href=
      "../plugin/highlight/monokai.css" id="highlight-theme">
    <link rel="stylesheet" href=
      "../css/style.css">
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <h2>Markov Chain Monte Carlo</h2>
        </section>

        <section>
          <h2>Statistical Mechanics</h2>
          <p>Last time, we talked about how importance sampling is secretly
          baked into statistical mechanics from the Boltzmann probability distribution:
          $$
            P(E_i) = \frac{e^{-E_i/k_B T}}{Z}
          $$</p>
          <p class="fragment">
            Monte Carlo methods are typically used to compute the expectation
            value of variables using samples drawn from the above distribution:
            $$
            \langle f \rangle = \frac{1}{Z} \sum_i f(E_i) e^{-E_i/k_B T}
            $$
          </p>
        </section>

        <section>
          <h2>Statistical Mechanics</h2>
          <p>But drawing random states following the Boltzmann distribution is
          no simple task!</p>
          <p class="fragment">
            Consider a system with 100 electrons, each with two possible spin
            configurations: up or down. The total number of states is $2^{100}$, which is
            approximately $10^{30}$.
          </p>
          <p class="fragment">
            If we want to sample the Boltzmann distribution, we need to sample
            from $10^{30}$ discrete states using their energies!
          </p>
        </section>

        <section>
          <h2>The Ising Model</h2>
          <p>An example of such a system is the so-called Ising model:
          $$
            H = -J \sum_{\langle i, j \rangle} s_i s_j
          $$</p>
          <p class="fragment">
            $H$ is the Hamiltonian of the system. $J$ is the coupling constant,
            which determines the strength of the interaction between neighboring
            spins. $s_i$ is the spin of the $i$th electron which may be $+1$ or
            $-1$, and the sum $\langle i,j\rangle$ is over all neighboring pairs
            of electrons.
          </p>
          <p class="fragment">
            The Ising model is a simple model of ferromagnetism, where the spin alignment
            leads to lower overall energy.
          </p>
        </section>

        <section>
          <h2>The Ising Model</h2>
          <p>For the Ising model, the energy of a state is given by the Hamiltonian:
          $$
            E = H = -J \sum_{\langle i, j \rangle} s_i s_j
          $$</p>
          <p class="fragment">
            For example, if all the spins are aligned, then the energy is minimized
            and the system is in the lowest energy state. At finite temperature however,
            the system will have a nonzero probability of being in higher energy states.
          </p>
        </section>

        <section>
          <h2>The Ising Model</h2>
          <p>How to generate a state for the Ising model from the Boltzmann distribution?</p>
        </section>

        <section>
          <h2>Markov Chain</h2>
          <p>Since constructing a state from scratch is quite expensive, we can
            reformulate the problem to look at the <em>transition</em> between states.</p>
          <p>A Markov Chain is a sequence of states where the transition
          probability from state $i$ to state $j$ in one step is independent
          from the history of the sequence.</p>
        </section>

        <section>
          <h2>Markov Chain</h2>
          <p>Starting from some arbitrary initial state, we construct a sequence
          of states $S_1$, $S_2$, $S_3$, $\dots$ If at step $n$ the system is in
          state $i$, then the probability of the system to be in state $j$ at step $n+1$ is the
            <em>transition probability</em> $T_{ij}$.</p>
          <div class="fragment">
            <p>$T_{ij}$ should satisfy some properties:</p>
            <ul>
              <li class="fragment">$\sum_j T_{ij} = 1$. The total probability of transitioning to all other states
              should be 1.</li>
              <li class="fragment">$P_iT_{ij} = P_jT_{ji}$. This is called the <em>detailed
              balance equation</em>. It ensures that eventually all states will be visited according
              to the right probability.</li>
            </ul>
          </div>
        </section>

        <section>
          <h2>Markov Chain</h2>
          <p>The detailed balance condition for the Boltzmann distribution reads:
          $$
            \frac{T_{ij}}{T_{ji}} = \frac{P(E_j)}{P(E_i)} = e^{-(E_j - E_i)/k_B T}
          $$</p>
          <p class="fragment">How to construct such a transition probability?</p>
        </section>

        <section>
          <h2>Markov Chain</h2>
          <p>The common way to construct $T_{ij}$ is to use something similar to
          rejection sampling. We propose a random state $j$ according to some
          proposal distribution, and accept the move with the following
          probability:
            $$
            P_a = \begin{cases} 1 & \text{if }E_i > E_j \\
            e^{-(E_j - E_i)/k_B T} & \text{if }E_i < E_j \end{cases}
            $$</p>
          <p class="fragment">If the move is rejected, then the system stays at state $i$. This ensures
          that $\sum_j T_{ij} = 1$.</p>
        </section>

        <section>
          <h2>Markov Chain</h2>
          <p>Does this scheme satisfy the detailed balance equation?</p>
          <p class="fragment">
            Suppose $E_j > E_i$, and there are $M$ such possible states $j$:
            $$
            T_{ij} = \frac{1}{M}e^{-(E_j - E_i)/k_B T},\quad T_{ji} = \frac{1}{M},\quad \frac{T_{ij}}{T_{ji}} = e^{-(E_j - E_i)/k_B T}
            $$
          </p>
          <p class="fragment">
            Suppose $E_j < E_i$, and there are $M$ such possible states $j$:
            $$
            T_{ij} = \frac{1}{M},\quad T_{ji} = \frac{1}{M}e^{-(E_i - E_j)/k_B T},\quad \frac{T_{ij}}{T_{ji}} = e^{-(E_j - E_i)/k_B T}
            $$
          </p>
        </section>

        <section>
          <h2>Markov Chain</h2>
          <p>Constructing this kind of Markov Chain using the given transition
            probability is called the <em>Metropolis-Hastings algorithm</em>.</p>
          <p class="fragment">
            Starting at an arbitrary state, the Markov Chain will eventually come into <em>thermal equilibrium</em>.
            The distribution of states will satisfy the Boltzmann distribution.
          </p>
        </section>

        <section>
          <h2>Markov Chain Monte Carlo</h2>
          <p>The Metropolis-Hastings MCMC algorithm can be summarized as the following:</p>
          <ul>
            <li class="fragment">Choose a random initial state</li>
            <li class="fragment">Select a new state to move to according to a proposal distribution</li>
            <li class="fragment">Compute the probability $P_a$ of accepting this move</li>
            <li class="fragment">Draw a uniform random variable $u$ between $0$
            and $1$, accept the move if $u < P_a$. Otherwise stay at the current state</li>
            <li class="fragment">Go to the second step and repeat this procedure</li>
          </ul>
        </section>

        <section>
          <h2>Markov Chain Monte Carlo</h2>
          <p>MCMC simulation history for the 2D Ising model at finite temperature:</p>
          <img src="mcmc.png" width="55%">
        </section>

        <section>
          <h2>Markov Chain Monte Carlo</h2>
          <p>MCMC typically takes some number of iterations to reach thermal
          equilibrium. The distribution before equilibrium is unreliable.</p>
          <p class="fragment">
            Outside of statistical physics, this initial period is called burn-in.
          </p>
          <p class="fragment">
            It is typically hard to predict how long the burn-in period will be,
            so it is crucial to look at the results to determine whether an equilibrium is
            reached.
          </p>
        </section>

        <section>
          <h2>Markov Chain Monte Carlo</h2>
          <p>MCMC is a very general method that can be used to sample from
          arbitrary probability distributions. For continuous distributions, the
          Markov Chain still consists of discrete steps $x_1$, $x_2$, $x_3$,
          $\dots$</p>
          <p class="fragment">
            The general Metropolis-Hastings algorithm consists of a "proposal distribution" $g(x,
            x_n)$ of $x$ given $x_n$, and an "acceptance probability" $A(x, x_n)$.
          </p>
        </section>

        <section>
          <h2>Markov Chain Monte Carlo</h2>
          <p>The general recipe of the Metropolis algorithm is:</p>
          <ul>
            <li class="fragment">Given the variable value $x_n$ at step $n$, generate a new
            candidate $x$ from the proposal distribution $g(x, x_n)$.</li>
            <li class="fragment">Compute the acceptance probability:
              $$
              A(x, x_n) = \min\left(1, \frac{P(x)}{P(x_n)}\frac{g(x_n, x)}{g(x, x_n)}\right)
              $$
            </li>
            <li class="fragment">Generate a uniform random number $0 < u < 1$.
            If $u < A(x, x_n)$ then accept the new value as $x_{n+1}$. Otherwise reject it and
            do nothing.</li>
          </ul>
        </section>

        <section>
          <h2>Markov Chain Monte Carlo</h2>
          <p>For general problems, choosing the proposal distribution $g(x,
          x_n)$ is often an art. When chosen well, the Markov Chain can converge to the equilibrium
          very quickly. When chosen badly, it never reaches the desired equilibrium.</p>
          <p class="fragment">As a result, it is often not possible to directly predict the number of
          steps required in the burn-in period. Trial-and-error is often needed.</p>
        </section>

        <section>
          <h2>Markov Chain Monte Carlo</h2>
          <p>Once a Markov Chain is burned-in, it can be used to easily generate
          a series of random variables from the target distribution. These random variables can then be
          used to compute the desired statistical properties of the distribution.</p>
          <p class="fragment">
            Back in statistical mechanics, the series of states after reaching thermal equilibrium is
            an excellent collection of sampled states that can be used to compute the expectation value
            of various physical quantities:
            $$
            \langle X\rangle = \frac{1}{N}\sum_i X_i
            $$
          </p>
        </section>

      </div>
    </div>
    <!-- <script src="js/head.min.js"></script> -->
    <!-- <script>
         head.js(
         "vendor/jquery.min.js",
         // "vendor/three.js/build/three.min.js",
         // "vendor/three.js/examples/js/controls/OrbitControls.js",
         /* "vendor/socket.io-client/dist/socket.io.js", */
         // "vendor/THREE.MeshLine.js",
         /* "vendor/dat.gui.min.js",*/
         "vendor/EventEmitter.js",
         // "js/samples.js",
         );
         </script> -->
    <script src="../dist/reveal.js"></script>
    <script src="../plugin/math/math.js"></script>
    <script src="../plugin/notes/notes.js"></script>
    <script src="../plugin/markdown/markdown.js"></script>
    <script src="../plugin/highlight/highlight.js"></script>

    <script>
      // More info about initialization & config:
      // - https://revealjs.com/initialization/
      // - https://revealjs.com/config/
      Reveal.initialize({
        transition: "fade",
        slideNumber: true,
        history: true,
        center: true,
        width: "100%",
        height: "100%",
        disableLayout: false,
        navigationMode: "default",
        // minScale: 1,
        // maxScale: 1,
        margin: 0.2,
        padding: 0,
        hash: true,
        math: {
          mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          // pass other options into `MathJax.Hub.Config()`
          TeX: { Macros: { RR: "{\\bf R}" } }
        },
        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath ]
      });
      Reveal.configure({
        keyboard: {
          81: null, // Do not respond to Q
          // 32: null, // Do not respond to space
        }
      });
      // Reveal.addEventListener("slidechanged", function(event) {
      //   var foot1 = document.getElementById("footer1");
      //   var foot2 = document.getElementById("footer2");
      //
      //   if (Reveal.getIndices().h === 0) {
      //     foot1.style.display = "none";
      //     foot2.style.display = "none";
      //   } else {
      //     foot1.style.display = "block";
      //     foot2.style.display = "block";
      //   }
      // });
      //
    </script>
  </body>
</html>
